"""
CareerNavigator LangGraph èŠ‚ç‚¹å®ç°
åŸºäºé˜¿é‡Œäº‘ç™¾ç‚¼APIçš„åŸå­åŒ–èŠ‚ç‚¹è®¾è®¡
"""

import uuid
import json
import re
from datetime import datetime
from typing import Dict, Any, List

from src.models.career_state import (
    CareerNavigatorState, AgentTask, AgentOutput, AgentStatus, 
    WorkflowStage, StateUpdater, UserFeedback, UserSatisfactionLevel
)
from src.services.llm_service import llm_service, call_mcp_api


def parse_llm_json_content(content: str) -> Dict[str, Any]:
    """
    æ™ºèƒ½è§£æLLMè¿”å›çš„JSONå†…å®¹ï¼Œå¤„ç†å¤šç§æ ¼å¼
    
    Args:
        content: LLMè¿”å›çš„åŸå§‹å†…å®¹
        
    Returns:
        è§£æåçš„å­—å…¸å¯¹è±¡
        
    Raises:
        json.JSONDecodeError: å½“æ‰€æœ‰è§£ææ–¹æ³•éƒ½å¤±è´¥æ—¶
    """
    if not content or not isinstance(content, str):
        raise json.JSONDecodeError("å†…å®¹ä¸ºç©ºæˆ–æ ¼å¼é”™è¯¯", content or "", 0)
    
    content = content.strip()
    
    # æ–¹æ³•1: ç›´æ¥è§£æJSON
    try:
        return json.loads(content)
    except json.JSONDecodeError:
        pass
    
    # æ–¹æ³•2: æå–```jsonä»£ç å—ä¸­çš„å†…å®¹
    json_block_pattern = r'```json\s*(.*?)\s*```'
    json_match = re.search(json_block_pattern, content, re.DOTALL | re.IGNORECASE)
    if json_match:
        try:
            json_content = json_match.group(1).strip()
            return json.loads(json_content)
        except json.JSONDecodeError:
            pass
    
    # æ–¹æ³•3: æå–ä»»æ„ä»£ç å—ä¸­çš„å†…å®¹ï¼ˆå¯èƒ½æ˜¯```æ²¡æœ‰æŒ‡å®šè¯­è¨€ï¼‰
    code_block_pattern = r'```\s*(.*?)\s*```'
    code_match = re.search(code_block_pattern, content, re.DOTALL)
    if code_match:
        try:
            json_content = code_match.group(1).strip()
            return json.loads(json_content)
        except json.JSONDecodeError:
            pass
    
    # æ–¹æ³•4: æå–{}åŒ…å›´çš„JSONå†…å®¹
    if '{' in content and '}' in content:
        try:
            # æ‰¾åˆ°ç¬¬ä¸€ä¸ª{å’Œæœ€åä¸€ä¸ª}
            start = content.find('{')
            end = content.rfind('}') + 1
            json_content = content[start:end]
            return json.loads(json_content)
        except json.JSONDecodeError:
            pass
    
    # æ–¹æ³•5: å°è¯•ç§»é™¤å¯èƒ½çš„å‰åç¼€æ–‡æœ¬ï¼Œæå–JSONéƒ¨åˆ†
    lines = content.split('\n')
    json_lines = []
    in_json = False
    brace_count = 0
    
    for line in lines:
        stripped_line = line.strip()
        if '{' in stripped_line and not in_json:
            in_json = True
            json_lines.append(line)
            brace_count += stripped_line.count('{') - stripped_line.count('}')
        elif in_json:
            json_lines.append(line)
            brace_count += stripped_line.count('{') - stripped_line.count('}')
            if brace_count == 0:
                break
    
    if json_lines:
        try:
            json_content = '\n'.join(json_lines)
            return json.loads(json_content)
        except json.JSONDecodeError:
            pass
    
    # æ–¹æ³•6: å¤„ç†æˆªæ–­çš„JSON (å°è¯•è¡¥é½æ‹¬å·)
    try:
        # æå–æœ€å¤–å±‚çš„ { } å†…å®¹
        start = content.find('{')
        if start != -1:
            json_part = content[start:]
            # ç§»é™¤æœ«å°¾çš„éJSONå­—ç¬¦ï¼ˆå¦‚ ```ï¼‰
            json_part = re.sub(r'```.*$', '', json_part, flags=re.DOTALL).strip()
            
            # ä¿®å¤å¸¸è§çš„åˆ—è¡¨æœªé—­åˆé—®é¢˜: "key": ["val" \n "next_key": -> "key": ["val"], \n "next_key":
            # è¿™ç§é”™è¯¯å¸¸å‡ºç°åœ¨LLMè¾“å‡ºä¸­ï¼Œå®ƒå¼€å¯äº†ä¸€ä¸ªåˆ—è¡¨ä½†å¿˜è®°å…³é—­å°±ç›´æ¥å†™ä¸‹ä¸€ä¸ªé”®å€¼å¯¹äº†
            json_part = re.sub(r'(\[[^\]]*?)\s*\n\s*(\s*\"[\w_]+\"\s*:\s*)', r'\1], \n \2', json_part)
            
            # ä¿®å¤ç¼ºå¤±é€—å·çš„é—®é¢˜: "key1": "val1" \n "key2": "val2" -> "key1": "val1", \n "key2": "val2"
            # åŒ¹é…æ¨¡å¼ï¼šä¸€ä¸ªå€¼åé¢ç´§è·Ÿæ¢è¡Œå’Œä¸‹ä¸€ä¸ªé”®åï¼Œä½†ä¸­é—´æ²¡æœ‰é€—å·
            json_part = re.sub(r'(\"(?:[^\"\\]|\\.)*\"\s*:\s*(?:\"(?:[^\"\\]|\\.)*\"|\d+|true|false|null|\[(?:[^\[\]]|\[[^\[\]]*\])*\]|\{(?:[^{}]|\{[^{}]*\})*\}))\s*\n\s*(\"(?:[^\"\\]|\\.)*\"\s*:\s*)', r'\1, \n \2', json_part)

            # å°è¯•ä¿®å¤è¢«æˆªæ–­çš„å­—ç¬¦ä¸²ï¼ˆå¦‚æœæœ€åä¸€è¡Œæ²¡æœ‰é—­åˆå¼•å·ï¼‰
            # æŸ¥æ‰¾æœ€åä¸€ä¸ªæœªé—­åˆçš„å¼•å·
            last_quote = json_part.rfind('"')
            if last_quote != -1:
                # æ£€æŸ¥è¿™ä¸ªå¼•å·æ˜¯å¦æ˜¯é—­åˆå¼•å·
                # ç®€å•é€»è¾‘ï¼šå¦‚æœå¼•å·åé¢ç´§è·Ÿçš„æ˜¯ , } ] æˆ–ç©ºç™½ï¼Œåˆ™è®¤ä¸ºæ˜¯é—­åˆçš„
                remaining = json_part[last_quote+1:].strip()
                if remaining and not any(c in remaining for c in [',', '}', ']', ':']):
                    # å¯èƒ½æ˜¯æˆªæ–­åœ¨å­—ç¬¦ä¸²ä¸­é—´ï¼Œå°è¯•è¡¥é½å¼•å·
                    json_part += '"'
            
            # ç»Ÿè®¡æ‹¬å·
            open_braces = json_part.count('{')
            close_braces = json_part.count('}')
            open_brackets = json_part.count('[')
            close_brackets = json_part.count(']')
            
            # è¡¥é½ç¼ºå¤±çš„æ‹¬å·
            fixed_json = json_part
            if open_brackets > close_brackets:
                fixed_json += ']' * (open_brackets - close_brackets)
            if open_braces > close_braces:
                fixed_json += '}' * (open_braces - close_braces)
                
            return json.loads(fixed_json)
    except:
        pass
    
    # å¦‚æœæ‰€æœ‰æ–¹æ³•éƒ½å¤±è´¥ï¼ŒæŠ›å‡ºå¼‚å¸¸
    raise json.JSONDecodeError(f"æ— æ³•è§£æJSONå†…å®¹ã€‚åŸå§‹å†…å®¹: {content[:200]}...", content, 0)


from langchain_core.runnables import RunnableConfig

def coordinator_node(state: CareerNavigatorState, config: RunnableConfig = None) -> Dict[str, Any]:
    """
    åè°ƒå‘˜èŠ‚ç‚¹ (å…¥å£ç‚¹)
    
    èŒè´£:
    1. æ£€æŸ¥ç”¨æˆ·çš„åˆå§‹è¯·æ±‚ã€‚
    2. è°ƒç”¨LLMåˆ¤æ–­ç”¨æˆ·çš„èŒä¸šç›®æ ‡æ˜¯å¦å·²ç»æ˜ç¡®ã€‚
    3. æ ¹æ®åˆ¤æ–­ç»“æœï¼Œå†³å®šä¸‹ä¸€ä¸ªæµç¨‹èŠ‚ç‚¹ã€‚
    """
    print("=" * 60)
    print("ğŸš€ æ­£åœ¨æ‰§è¡Œ: coordinator_node")
    print("=" * 60)
    
    # è·å–æµå¼å›è°ƒ
    stream_callback = None
    if config and "configurable" in config and "stream_callback" in config["configurable"]:
        stream_callback = config["configurable"]["stream_callback"]
        if stream_callback:
            stream_callback(json.dumps({"node": "coordinator", "status": "start"}))

    # æ£€æŸ¥ç”¨æˆ·æ»¡æ„åº¦ï¼Œå¦‚æœå·²ç»æœ‰äº†æ»¡æ„åº¦åé¦ˆï¼Œè¯´æ˜æ˜¯ç‚¹å‡»äº†â€œæ»¡æ„â€æˆ–â€œä¸æ»¡æ„â€åé‡æ–°è¿›å…¥çš„
    # ä¼˜å…ˆä» state ç›´æ¥è·å–ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™å°è¯•ä» user_feedback_history è·å–
    current_satisfaction = state.get("current_satisfaction")
    if current_satisfaction is None and state.get("user_feedback_history"):
        latest_feedback = state["user_feedback_history"][-1]
        # å…¼å®¹å­—å…¸å’Œå¯¹è±¡æ ¼å¼
        if isinstance(latest_feedback, dict):
            current_satisfaction = latest_feedback.get("satisfaction_level")
        else:
            current_satisfaction = getattr(latest_feedback, "satisfaction_level", None)
        print(f"â„¹ï¸ ä»å†å²è®°å½•ä¸­æ¢å¤æ»¡æ„åº¦çŠ¶æ€: {current_satisfaction}")

    current_stage = state.get("current_stage")
    
    print(f"ğŸ” Coordinator æ£€æŸ¥çŠ¶æ€: stage={current_stage}, satisfaction={current_satisfaction}")
    
    # å¦‚æœå·²ç»æœ‰ç»¼åˆæŠ¥å‘Šä¸”å¤„äºç­‰å¾…åé¦ˆé˜¶æ®µï¼Œä½†æ²¡æœ‰æ–°çš„æ»¡æ„åº¦è¾“å…¥ï¼Œè¯´æ˜å¯èƒ½æ˜¯é‡å¤è§¦å‘ï¼Œç›´æ¥ç»“æŸ
    if current_satisfaction is None and current_stage == WorkflowStage.USER_FEEDBACK and state.get("integrated_report"):
        print("â¸ï¸ å½“å‰å¤„äºç­‰å¾…ç”¨æˆ·åé¦ˆé˜¶æ®µï¼Œä¸”å·²æœ‰æŠ¥å‘Šï¼Œè·³è¿‡é‡å¤æ‰§è¡Œ")
        if stream_callback:
            stream_callback(json.dumps({"node": "coordinator", "content": "æ­£åœ¨ç­‰å¾…æ‚¨çš„åé¦ˆ..."}))
            stream_callback(json.dumps({"node": "coordinator", "status": "end"}))
        return {"next_node": "end"}
    
    # å¦‚æœå¤„äºåç»­é˜¶æ®µï¼Œè‡ªåŠ¨è·³è½¬
    if current_satisfaction is None:
        if current_stage == WorkflowStage.GOAL_DECOMPOSITION:
            print("â© è‡ªåŠ¨è·³è½¬åˆ°ç›®æ ‡æ‹†è§£é˜¶æ®µ")
            updates = {"next_node": "goal_decomposer"}
            return updates
        elif current_stage == WorkflowStage.SCHEDULE_PLANNING:
            print("â© è‡ªåŠ¨è·³è½¬åˆ°æ—¥ç¨‹è§„åˆ’é˜¶æ®µ")
            updates = {"next_node": "scheduler"}
            return updates
        elif current_stage == WorkflowStage.COMPLETED:
            print("âœ… æµç¨‹å·²å®Œæˆï¼Œç›´æ¥ç»“æŸ")
            updates = {"next_node": "end"}
            return updates

    if current_satisfaction is not None:
        # å¤„ç†åˆ†ææŠ¥å‘Šé˜¶æ®µçš„åé¦ˆ
        if current_stage == WorkflowStage.USER_FEEDBACK:
            if current_satisfaction in [UserSatisfactionLevel.SATISFIED, UserSatisfactionLevel.VERY_SATISFIED]:
                print(f"âœ… æ£€æµ‹åˆ°ç”¨æˆ·å·²æ»¡æ„åˆ†ææŠ¥å‘Š({current_satisfaction.value})ï¼Œç›´æ¥è·³è½¬åˆ°ç›®æ ‡æ‹†åˆ†é˜¶æ®µ")
                updates = StateUpdater.update_stage(state, WorkflowStage.GOAL_DECOMPOSITION)
                updates["next_node"] = "goal_decomposer"
                updates["current_satisfaction"] = None
                if stream_callback:
                    stream_callback(json.dumps({"node": "coordinator", "content": "æ£€æµ‹åˆ°æ‚¨å·²ç¡®è®¤æŠ¥å‘Šï¼Œæ­£åœ¨è¿›å…¥ç›®æ ‡æ‹†è§£é˜¶æ®µ..."}))
                    stream_callback(json.dumps({"node": "coordinator", "status": "end"}))
                return updates
            elif current_satisfaction in [UserSatisfactionLevel.DISSATISFIED, UserSatisfactionLevel.VERY_DISSATISFIED]:
                print(f"ğŸ”„ æ£€æµ‹åˆ°ç”¨æˆ·ä¸æ»¡æ„åˆ†ææŠ¥å‘Š({current_satisfaction.value})ï¼Œé‡æ–°è¿›å…¥ç­–ç•¥åˆ¶å®šé˜¶æ®µ")
                updates = StateUpdater.update_stage(state, WorkflowStage.PLANNING)
                updates["next_node"] = "planner"
                updates["current_satisfaction"] = None
                if stream_callback:
                    stream_callback(json.dumps({"node": "coordinator", "content": "æ£€æµ‹åˆ°æ‚¨å¯¹æŠ¥å‘Šæœ‰ä¿®æ”¹æ„è§ï¼Œæ­£åœ¨é‡æ–°ä¸ºæ‚¨åˆ†æ..."}))
                    stream_callback(json.dumps({"node": "coordinator", "status": "end"}))
                return updates
        
        # å¤„ç†æœ€ç»ˆç¡®è®¤é˜¶æ®µçš„åé¦ˆ
        elif current_stage == WorkflowStage.FINAL_CONFIRMATION:
            if current_satisfaction in [UserSatisfactionLevel.SATISFIED, UserSatisfactionLevel.VERY_SATISFIED]:
                print(f"âœ… æ£€æµ‹åˆ°ç”¨æˆ·å·²æ»¡æ„æœ€ç»ˆè®¡åˆ’({current_satisfaction.value})ï¼Œæµç¨‹ç»“æŸ")
                updates = StateUpdater.update_stage(state, WorkflowStage.COMPLETED)
                updates["next_node"] = "end"
                updates["current_satisfaction"] = None
                if stream_callback:
                    stream_callback(json.dumps({"node": "coordinator", "content": "æ„Ÿè°¢æ‚¨çš„ç¡®è®¤ï¼ŒèŒä¸šè§„åˆ’æµç¨‹å·²åœ†æ»¡å®Œæˆï¼"}))
                    stream_callback(json.dumps({"node": "coordinator", "status": "end"}))
                return updates
            elif current_satisfaction in [UserSatisfactionLevel.DISSATISFIED, UserSatisfactionLevel.VERY_DISSATISFIED]:
                print(f"ğŸ”„ æ£€æµ‹åˆ°ç”¨æˆ·ä¸æ»¡æ„æœ€ç»ˆè®¡åˆ’({current_satisfaction.value})ï¼Œè¿”å›ç›®æ ‡æ‹†åˆ†é˜¶æ®µé‡æ–°è°ƒæ•´")
                updates = StateUpdater.update_stage(state, WorkflowStage.GOAL_DECOMPOSITION)
                updates["next_node"] = "goal_decomposer"
                updates["current_satisfaction"] = None
                if stream_callback:
                    stream_callback(json.dumps({"node": "coordinator", "content": "æ£€æµ‹åˆ°æ‚¨å¯¹è®¡åˆ’æœ‰ä¿®æ”¹æ„è§ï¼Œæ­£åœ¨ä¸ºæ‚¨é‡æ–°è°ƒæ•´ç›®æ ‡æ‹†è§£..."}))
                    stream_callback(json.dumps({"node": "coordinator", "status": "end"}))
                return updates

    messages = state.get("messages", [])
    user_request = messages[-1].content if messages else ""
    print(f"ğŸ“ ç”¨æˆ·è¯·æ±‚: {user_request}")
    
    # å®‰å…¨è·å–ç”¨æˆ·ç”»åƒ
    user_profile = state.get("user_profile")
    if not user_profile:
        print("âš ï¸ è­¦å‘Š: æœªæ‰¾åˆ°ç”¨æˆ·ç”»åƒï¼Œä½¿ç”¨é»˜è®¤å€¼")
        user_profile = {
            "user_id": "unknown",
            "age": 0,
            "education_level": "æœªçŸ¥",
            "work_experience": 0,
            "current_position": "æœªçŸ¥",
            "industry": "æœªçŸ¥",
            "skills": [],
            "interests": [],
            "career_goals": "æœªçŸ¥",
            "location": "æœªçŸ¥",
            "salary_expectation": "æœªçŸ¥"
        }

    # è°ƒç”¨ç™¾ç‚¼APIåˆ†æç›®æ ‡æ˜ç¡®åº¦
    llm_response = llm_service.analyze_career_goal_clarity(
        user_request, 
        user_profile,
        stream_callback=lambda x: stream_callback(json.dumps({"node": "coordinator", "content": x})) if stream_callback else None
    )
    
    if stream_callback:
        stream_callback(json.dumps({"node": "coordinator", "status": "end"}))
    
    print(f"ğŸ¤– LLMåŸå§‹å“åº”: {json.dumps(llm_response, ensure_ascii=False, indent=2)}")
    
    if llm_response.get("success"):
        try:
            # ä½¿ç”¨æ™ºèƒ½JSONè§£æ
            analysis = parse_llm_json_content(llm_response["content"])
            is_goal_clear = analysis.get("is_goal_clear", False)
            clarity_score = analysis.get("clarity_score", 0)
            
            print(f"ğŸ“Š ç›®æ ‡æ˜ç¡®åº¦åˆ†æç»“æœ:")
            print(f"   - ç›®æ ‡æ˜¯å¦æ˜ç¡®: {is_goal_clear}")
            print(f"   - æ˜ç¡®åº¦è¯„åˆ†: {clarity_score}")
            print(f"   - è¯¦ç»†åˆ†æ: {json.dumps(analysis, ensure_ascii=False, indent=2)}")
            
            if is_goal_clear:
                print("âœ… åˆ¤æ–­ï¼šç›®æ ‡æ˜ç¡®ï¼Œç›´æ¥è¿›å…¥ç›®æ ‡æ‹†åˆ†ã€‚")
                # æ›´æ–°çŠ¶æ€ï¼Œç›´æ¥è¿›å…¥ç›®æ ‡æ‹†åˆ†é˜¶æ®µ
                updates = StateUpdater.update_stage(state, WorkflowStage.GOAL_DECOMPOSITION)
                updates["next_node"] = "goal_decomposer"
                updates["cached_data"] = {"goal_analysis": analysis}
                
                print(f"ğŸ”„ çŠ¶æ€æ›´æ–°: {json.dumps(updates, ensure_ascii=False, indent=2, default=str)}")
                return updates
            else:
                print("ğŸ”„ åˆ¤æ–­ï¼šç›®æ ‡ä¸æ˜ç¡®ï¼Œéœ€è¦è¿›è¡Œè§„åˆ’å’Œåˆ†æã€‚")
                # æ›´æ–°çŠ¶æ€ï¼Œè¿›å…¥ç­–ç•¥åˆ¶å®šé˜¶æ®µ
                updates = StateUpdater.update_stage(state, WorkflowStage.PLANNING)
                updates["next_node"] = "planner"
                updates["cached_data"] = {"goal_analysis": analysis}
                
                print(f"ğŸ”„ çŠ¶æ€æ›´æ–°: {json.dumps(updates, ensure_ascii=False, indent=2, default=str)}")
                return updates
        except json.JSONDecodeError as e:
            print(f"âŒ LLMå“åº”è§£æå¤±è´¥: {str(e)}")
            print(f"ğŸ“„ åŸå§‹å“åº”å†…å®¹: {llm_response['content'][:300]}...")
            print("ğŸ”„ é»˜è®¤è¿›å…¥è§„åˆ’é˜¶æ®µ")
            updates = StateUpdater.update_stage(state, WorkflowStage.PLANNING)
            updates["next_node"] = "planner"
            
            print(f"ğŸ”„ çŠ¶æ€æ›´æ–°: {json.dumps(updates, ensure_ascii=False, indent=2, default=str)}")
            return updates
    else:
        print(f"âŒ LLMè°ƒç”¨å¤±è´¥: {llm_response.get('error')}")
        # é»˜è®¤è¿›å…¥è§„åˆ’é˜¶æ®µ
        updates = StateUpdater.update_stage(state, WorkflowStage.PLANNING)
        updates["next_node"] = "planner"
        
        print(f"ğŸ”„ çŠ¶æ€æ›´æ–°: {json.dumps(updates, ensure_ascii=False, indent=2, default=str)}")
        return updates


def planner_node(state: CareerNavigatorState, config: RunnableConfig = None) -> Dict[str, Any]:
    """
    è®¡åˆ’å‘˜èŠ‚ç‚¹
    
    èŒè´£:
    1. å½“ç”¨æˆ·ç›®æ ‡ä¸æ˜ç¡®æ—¶ï¼Œåˆ¶å®šä¸€ä¸ªè¯¦ç»†çš„åˆ†æç­–ç•¥ã€‚
    2. å°†ç­–ç•¥å­˜å…¥Stateä¸­ï¼Œä¾›åç»­èŠ‚ç‚¹ä½¿ç”¨ã€‚
    """
    print("=" * 60)
    print("ğŸ“‹ æ­£åœ¨æ‰§è¡Œ: planner_node")
    print("=" * 60)
    
    # è·å–æµå¼å›è°ƒ
    stream_callback = None
    if config and "configurable" in config and "stream_callback" in config["configurable"]:
        stream_callback = config["configurable"]["stream_callback"]
        if stream_callback:
            stream_callback(json.dumps({"node": "planner", "status": "start"}))

    user_profile = state["user_profile"]
    feedback_history = state["user_feedback_history"]
    
    print(f"ğŸ‘¤ ç”¨æˆ·ç”»åƒ: {json.dumps(dict(user_profile), ensure_ascii=False, indent=2)}")
    print(f"ğŸ’¬ åé¦ˆå†å²: {len(feedback_history)} æ¡è®°å½•")
    
    # è°ƒç”¨ç™¾ç‚¼APIåˆ¶å®šåˆ†æç­–ç•¥
    llm_response = llm_service.create_analysis_strategy(
        user_profile, 
        feedback_history,
        stream_callback=lambda x: stream_callback(json.dumps({"node": "planner", "content": x})) if stream_callback else None
    )
    
    if stream_callback:
        stream_callback(json.dumps({"node": "planner", "status": "end"}))
    
    print(f"ğŸ¤– LLMåŸå§‹å“åº”: {json.dumps(llm_response, ensure_ascii=False, indent=2)}")
    
    # å‡†å¤‡æ›´æ–°ï¼ŒåŒæ—¶æ¸…é™¤æ»¡æ„åº¦çŠ¶æ€ï¼Œä»¥ä¾¿ä¸‹æ¬¡åé¦ˆå¾ªç¯
    updates = {"current_satisfaction": None}
    
    if llm_response.get("success"):
        try:
            # ä½¿ç”¨æ™ºèƒ½JSONè§£æ
            strategy = parse_llm_json_content(llm_response["content"])
            print(f"ğŸ“Š åˆ†æç­–ç•¥ç»“æœ: {json.dumps(strategy, ensure_ascii=False, indent=2)}")
            
            updates["planning_strategy"] = strategy.get("strategy_overview", "åˆ¶å®šä¸ªæ€§åŒ–èŒä¸šåˆ†æç­–ç•¥")
            print(f"ğŸ”„ çŠ¶æ€æ›´æ–°: {json.dumps(updates, ensure_ascii=False, indent=2)}")
            return updates
        except json.JSONDecodeError as e:
            print(f"âŒ ç­–ç•¥è§£æå¤±è´¥: {str(e)}")
            print(f"ğŸ“„ åŸå§‹å“åº”å†…å®¹: {llm_response['content'][:300]}...")
            print("ğŸ”„ ä½¿ç”¨é»˜è®¤ç­–ç•¥")
            updates["planning_strategy"] = "åˆ¶å®šä¸ªæ€§åŒ–èŒä¸šåˆ†æç­–ç•¥"
            print(f"ğŸ”„ çŠ¶æ€æ›´æ–°: {json.dumps(updates, ensure_ascii=False, indent=2)}")
            return updates
    else:
        print(f"âŒ ç­–ç•¥åˆ¶å®šå¤±è´¥: {llm_response.get('error')}")
        updates["planning_strategy"] = "åˆ¶å®šä¸ªæ€§åŒ–èŒä¸šåˆ†æç­–ç•¥"
        print(f"ğŸ”„ çŠ¶æ€æ›´æ–°: {json.dumps(updates, ensure_ascii=False, indent=2)}")
        return updates


def supervisor_node(state: CareerNavigatorState, config: RunnableConfig = None) -> Dict[str, Any]:
    """
    ç®¡ç†å‘˜èŠ‚ç‚¹
    
    èŒè´£:
    1. æ ¹æ® `planning_strategy` åˆ›å»ºå¹¶åˆ†å‘å¹¶è¡Œçš„åˆ†æä»»åŠ¡ã€‚
    2. ä¸ºæ¯ä¸ªä»»åŠ¡åˆ›å»ºä¸€ä¸ª AgentTask å¯¹è±¡ï¼Œå¹¶æ·»åŠ åˆ° State ä¸­ã€‚
    3. åœ¨è¿­ä»£æ—¶ï¼Œè€ƒè™‘ç”¨æˆ·åé¦ˆæ¥è°ƒæ•´åˆ†æç­–ç•¥ã€‚
    """
    print("=" * 60)
    print("ğŸ‘¨â€ğŸ’¼ æ­£åœ¨æ‰§è¡Œ: supervisor_node")
    print("=" * 60)
    
    # è·å–æµå¼å›è°ƒ
    stream_callback = None
    if config and "configurable" in config and "stream_callback" in config["configurable"]:
        stream_callback = config["configurable"]["stream_callback"]
        if stream_callback:
            stream_callback(json.dumps({"node": "supervisor", "status": "start"}))
            stream_callback(json.dumps({"node": "supervisor", "content": "æ­£åœ¨æ ¹æ®ç­–ç•¥åˆ†é…åˆ†æä»»åŠ¡..."}))

    plan = state.get("planning_strategy", "åˆ¶å®šä¸ªæ€§åŒ–èŒä¸šåˆ†æç­–ç•¥")
    print(f"ğŸ“‹ å½“å‰ç­–ç•¥: {plan}")
    
    # æ£€æŸ¥æ˜¯å¦æœ‰ç”¨æˆ·åé¦ˆéœ€è¦è€ƒè™‘
    feedback_history = state.get("user_feedback_history", [])
    latest_feedback = feedback_history[-1] if feedback_history else None
    
    # å¦‚æœæœ‰æœ€æ–°åé¦ˆï¼Œè°ƒæ•´åˆ†æé‡ç‚¹
    analysis_adjustments = {}
    if latest_feedback:
        feedback_text = latest_feedback.get("feedback_text") or ""
        print(f"ğŸ’¬ è€ƒè™‘ç”¨æˆ·åé¦ˆè¿›è¡Œè°ƒæ•´: {feedback_text}")
        
        # æ ¹æ®åé¦ˆè°ƒæ•´åˆ†æé‡ç‚¹
        if feedback_text and ("å¤§æ¨¡å‹" in feedback_text or "AI" in feedback_text):
            analysis_adjustments["focus_areas"] = ["AIæŠ€æœ¯èƒŒæ™¯", "å¤§æ¨¡å‹ç›¸å…³ç»éªŒ", "æŠ€æœ¯è½¬äº§å“è·¯å¾„"]
        if feedback_text and "å­¦ä¹ " in feedback_text:
            analysis_adjustments["focus_areas"] = analysis_adjustments.get("focus_areas", []) + ["å­¦ä¹ è·¯å¾„", "æŠ€èƒ½æå‡"]
    
    print(f"ğŸ¯ åˆ†æè°ƒæ•´: {json.dumps(analysis_adjustments, ensure_ascii=False, indent=2)}")
    
    # åŸºäºè®¡åˆ’å’Œåé¦ˆï¼Œåˆ›å»ºä¸‰ä¸ªå¹¶è¡Œä»»åŠ¡
    tasks = [
        AgentTask(
            task_id=str(uuid.uuid4()),
            agent_name="user_profiler_node",
            task_type="ä¸ªäººåˆ†æ",
            priority=1,
            description="æ‰§è¡Œè‡ªæˆ‘æ´å¯Ÿåˆ†æï¼Œç”Ÿæˆä¸ªäººèƒ½åŠ›ç”»åƒã€‚æ ¹æ®ç”¨æˆ·åé¦ˆé‡ç‚¹åˆ†æç›¸å…³æŠ€èƒ½ã€‚",
            input_data={
                "user_profile": state["user_profile"],
                "feedback_adjustments": analysis_adjustments,
                "iteration_count": state.get("iteration_count", 0)
            },
            status=AgentStatus.IDLE,
            created_at=datetime.now(),
            deadline=None,
            dependencies=None,
            started_at=None,
            completed_at=None
        ),
        AgentTask(
            task_id=str(uuid.uuid4()),
            agent_name="industry_researcher_node",
            task_type="è¡Œä¸šç ”ç©¶",
            priority=1,
            description="æ‰§è¡Œè¡Œä¸šè¶‹åŠ¿åˆ†æï¼Œç”Ÿæˆè¡Œä¸šæŠ¥å‘Šã€‚ç»“åˆç”¨æˆ·åé¦ˆè°ƒæ•´ç ”ç©¶é‡ç‚¹ã€‚",
            input_data={
                "target_industry": state["user_profile"].get("industry"),
                "feedback_adjustments": analysis_adjustments,
                "iteration_count": state.get("iteration_count", 0)
            },
            status=AgentStatus.IDLE,
            created_at=datetime.now(),
            deadline=None,
            dependencies=None,
            started_at=None,
            completed_at=None
        ),
        AgentTask(
            task_id=str(uuid.uuid4()),
            agent_name="job_analyzer_node",
            task_type="èŒä¸šåˆ†æ",
            priority=1,
            description="æ‰§è¡ŒèŒä¸šä¸å²—ä½åˆ†æï¼Œç”ŸæˆèŒä¸šå»ºè®®ã€‚æ ¹æ®ç”¨æˆ·åé¦ˆè°ƒæ•´èŒä¸šè·¯å¾„åˆ†æã€‚",
            input_data={
                "target_career": state["user_profile"].get("career_goals"),
                "feedback_adjustments": analysis_adjustments,
                "iteration_count": state.get("iteration_count", 0)
            },
            status=AgentStatus.IDLE,
            created_at=datetime.now(),
            deadline=None,
            dependencies=None,
            started_at=None,
            completed_at=None
        )
    ]
    
    print(f"ğŸ“‹ åˆ›å»ºäº† {len(tasks)} ä¸ªå¹¶è¡Œä»»åŠ¡:")
    for i, task in enumerate(tasks, 1):
        print(f"   {i}. {task['agent_name']} - {task['task_type']}")
        print(f"      æè¿°: {task['description']}")
        print(f"      è¾“å…¥æ•°æ®: {json.dumps(task['input_data'], ensure_ascii=False, indent=6, default=str)}")
    
    # æ›´æ–°çŠ¶æ€ï¼Œè¿›å…¥å¹¶è¡Œåˆ†æé˜¶æ®µ
    updated_state = StateUpdater.update_stage(state, WorkflowStage.PARALLEL_ANALYSIS)
    updated_state["agent_tasks"] = tasks
    
    # åœ¨ supervisor_node ç»“æŸå‰å‘é€ç»“æŸçŠ¶æ€
    if stream_callback:
        stream_callback(json.dumps({"node": "supervisor", "status": "end"}))
    
    print(f"ğŸ”„ çŠ¶æ€æ›´æ–°: {json.dumps(updated_state, ensure_ascii=False, indent=2, default=str)}")
    return updated_state


# --- å¹¶è¡Œåˆ†æèŠ‚ç‚¹ ---
def user_profiler_node(state: CareerNavigatorState, config: RunnableConfig = None) -> Dict[str, Any]:
    """ç”¨æˆ·å»ºæ¨¡èŠ‚ç‚¹ (å¹¶è¡Œ)"""
    print("=" * 60)
    print("ğŸ‘¤ æ­£åœ¨æ‰§è¡Œ: user_profiler_node")
    print("=" * 60)
    
    # è·å–æµå¼å›è°ƒ
    stream_callback = None
    if config and "configurable" in config and "stream_callback" in config["configurable"]:
        stream_callback = config["configurable"]["stream_callback"]
        if stream_callback:
            stream_callback(json.dumps({"node": "user_profiler", "status": "start"}))

    task = next((t for t in state["agent_tasks"] if t["agent_name"] == "user_profiler_node"), None)
    
    if not task:
        print("âŒ æœªæ‰¾åˆ°ç”¨æˆ·ç”»åƒåˆ†æä»»åŠ¡")
        if stream_callback:
            stream_callback(json.dumps({"node": "user_profiler", "status": "end"}))
        return StateUpdater.log_error(state, {"error": "æœªæ‰¾åˆ°ç”¨æˆ·ç”»åƒåˆ†æä»»åŠ¡"})
    
    print(f"ğŸ“‹ ä»»åŠ¡ä¿¡æ¯: {task['task_type']} - {task['description']}")
    
    # è·å–åˆ†æè°ƒæ•´å’Œè¿­ä»£ä¿¡æ¯
    input_data = task["input_data"]
    feedback_adjustments = input_data.get("feedback_adjustments", {})
    iteration_count = input_data.get("iteration_count", 0)
    
    print(f"ğŸ”„ è¿­ä»£æ¬¡æ•°: {iteration_count}")
    print(f"ğŸ¯ åé¦ˆè°ƒæ•´: {json.dumps(feedback_adjustments, ensure_ascii=False, indent=2)}")
    
    # æ„å»ºåˆ†æè¯·æ±‚ï¼ŒåŒ…å«åé¦ˆè°ƒæ•´
    analysis_request = {
        "user_profile": dict(input_data.get("user_profile", {})),
        "feedback_adjustments": feedback_adjustments,
        "focus_areas": feedback_adjustments.get("focus_areas", []),
        "is_iteration": iteration_count > 0,
        "improvement_notes": "ç»“åˆç”¨æˆ·åé¦ˆé‡æ–°åˆ†æç”¨æˆ·èƒ½åŠ›å’Œä¼˜åŠ¿"
    }
    
    print(f"ğŸ“¤ åˆ†æè¯·æ±‚: {json.dumps(analysis_request, ensure_ascii=False, indent=2, default=str)}")
    
    # è°ƒç”¨ç™¾ç‚¼APIè¿›è¡Œç”¨æˆ·ç”»åƒåˆ†æ
    llm_response = llm_service.analyze_user_profile(
        analysis_request["user_profile"],
        feedback_adjustments=analysis_request["feedback_adjustments"],
        stream_callback=lambda x: stream_callback(json.dumps({"node": "user_profiler", "content": x})) if stream_callback else None
    )
    
    if stream_callback:
        stream_callback(json.dumps({"node": "user_profiler", "status": "end"}))
    
    print(f"ğŸ¤– LLMåŸå§‹å“åº”: {json.dumps(llm_response, ensure_ascii=False, indent=2)}")
    
    if llm_response.get("success"):
        try:
            # ä½¿ç”¨æ™ºèƒ½JSONè§£æ
            result = parse_llm_json_content(llm_response["content"])
            print(f"ğŸ“Š ç”¨æˆ·ç”»åƒåˆ†æç»“æœ (è¿­ä»£{iteration_count}): {json.dumps(result, ensure_ascii=False, indent=2)}")
        except json.JSONDecodeError as e:
            result = {"error": f"å“åº”è§£æå¤±è´¥: {str(e)}", "raw_response": llm_response["content"][:500]}
            print(f"âŒ å“åº”è§£æå¤±è´¥: {result}")
    else:
        result = {"error": llm_response.get("error", "åˆ†æå¤±è´¥")}
        print(f"âŒ åˆ†æå¤±è´¥: {result}")
    
    # æ·»åŠ è¿­ä»£ä¿¡æ¯
    result["iteration_info"] = {
        "iteration_count": iteration_count,
        "adjustments_applied": feedback_adjustments,
        "analysis_timestamp": datetime.now().isoformat()
    }
    
    output = AgentOutput(
        agent_name="user_profiler_node",
        task_id=task["task_id"],
        output_type="ä¸ªäººç”»åƒ",
        content=result,
        confidence_score=0.8 + (0.1 * iteration_count),  # è¿­ä»£æå‡ç½®ä¿¡åº¦
        data_sources=["ç™¾ç‚¼API", "ç”¨æˆ·è¾“å…¥", "ç”¨æˆ·åé¦ˆ"],
        analysis_method="LLMåˆ†æ+åé¦ˆä¼˜åŒ–",
        timestamp=datetime.now(),
        quality_metrics={"completeness": 0.9, "accuracy": 0.8 + (0.1 * iteration_count)},
        recommendations=result.get("recommendations", []),
        warnings=None
    )
    
    updates = {
        "self_insight_result": result, 
        "agent_outputs": [output]  # è¿”å›å•ä¸ªè¾“å‡ºï¼Œç”±Annotatedè‡ªåŠ¨åˆå¹¶
    }
    
    print(f"ğŸ”„ çŠ¶æ€æ›´æ–°: {json.dumps(updates, ensure_ascii=False, indent=2, default=str)}")
    return updates


def industry_researcher_node(state: CareerNavigatorState, config: RunnableConfig = None) -> Dict[str, Any]:
    """è¡Œä¸šç ”ç©¶èŠ‚ç‚¹ (å¹¶è¡Œ)"""
    print("=" * 60)
    print("ğŸ¢ æ­£åœ¨æ‰§è¡Œ: industry_researcher_node")
    print("=" * 60)
    
    # è·å–æµå¼å›è°ƒ
    stream_callback = None
    if config and "configurable" in config and "stream_callback" in config["configurable"]:
        stream_callback = config["configurable"]["stream_callback"]
        if stream_callback:
            stream_callback(json.dumps({"node": "industry_researcher", "status": "start"}))

    task = next((t for t in state["agent_tasks"] if t["agent_name"] == "industry_researcher_node"), None)
    
    if not task:
        print("âŒ æœªæ‰¾åˆ°è¡Œä¸šç ”ç©¶ä»»åŠ¡")
        if stream_callback:
            stream_callback(json.dumps({"node": "industry_researcher", "status": "end"}))
        return StateUpdater.log_error(state, {"error": "æœªæ‰¾åˆ°è¡Œä¸šç ”ç©¶ä»»åŠ¡"})
    
    print(f"ğŸ“‹ ä»»åŠ¡ä¿¡æ¯: {task['task_type']} - {task['description']}")
    
    target_industry = task["input_data"].get("target_industry", "ç§‘æŠ€è¡Œä¸š")
    feedback_adjustments = task["input_data"].get("feedback_adjustments", {})
    iteration_count = task["input_data"].get("iteration_count", 0)
    
    print(f"ğŸ¢ ç›®æ ‡è¡Œä¸š: {target_industry}")
    print(f"ğŸ”„ è¿­ä»£æ¬¡æ•°: {iteration_count}")
    print(f"ğŸ¯ åé¦ˆè°ƒæ•´: {json.dumps(feedback_adjustments, ensure_ascii=False, indent=2)}")
    
    # æ„å»ºç ”ç©¶è¯·æ±‚ï¼ŒåŒ…å«åé¦ˆè°ƒæ•´
    research_request = {
        "target_industry": target_industry,
        "focus_areas": feedback_adjustments.get("focus_areas", []),
        "is_iteration": iteration_count > 0,
        "special_focus": "ç»“åˆç”¨æˆ·åé¦ˆï¼Œé‡ç‚¹å…³æ³¨AIå’Œå¤§æ¨¡å‹ç›¸å…³çš„è¡Œä¸šæœºä¼š" if "AI" in str(feedback_adjustments) else ""
    }
    
    print(f"ğŸ“¤ ç ”ç©¶è¯·æ±‚: {json.dumps(research_request, ensure_ascii=False, indent=2)}")
    
    # è°ƒç”¨ç™¾ç‚¼APIè¿›è¡Œè¡Œä¸šç ”ç©¶
    llm_response = llm_service.research_industry_trends(
        target_industry,
        stream_callback=lambda x: stream_callback(json.dumps({"node": "industry_researcher", "content": x})) if stream_callback else None
    )
    
    if stream_callback:
        stream_callback(json.dumps({"node": "industry_researcher", "status": "end"}))
    
    print(f"ğŸ¤– LLMåŸå§‹å“åº”: {json.dumps(llm_response, ensure_ascii=False, indent=2)}")
    
    if llm_response.get("success"):
        try:
            # ä½¿ç”¨æ™ºèƒ½JSONè§£æ
            result = parse_llm_json_content(llm_response["content"])
            print(f"ğŸ“Š è¡Œä¸šç ”ç©¶ç»“æœ (è¿­ä»£{iteration_count}): {json.dumps(result, ensure_ascii=False, indent=2)}")
        except json.JSONDecodeError as e:
            result = {"error": f"å“åº”è§£æå¤±è´¥: {str(e)}", "raw_response": llm_response["content"][:500]}
            print(f"âŒ å“åº”è§£æå¤±è´¥: {result}")
    else:
        result = {"error": llm_response.get("error", "ç ”ç©¶å¤±è´¥")}
        print(f"âŒ ç ”ç©¶å¤±è´¥: {result}")
    
    # è¡¥å……æ¨¡æ‹Ÿçš„å¸‚åœºæ•°æ®
    mcp_data = call_mcp_api("industry_data", task["input_data"])
    # print(f"ğŸ”— MCP industry_data ç»“æœ: {json.dumps(mcp_data, ensure_ascii=False, indent=2)}")
    #å°±ä¸šå¸‚åœºçˆ¬å–ç»“æœ
    result["market_data"] = mcp_data
    
    # æ·»åŠ è¿­ä»£ä¿¡æ¯
    result["iteration_info"] = {
        "iteration_count": iteration_count,
        "adjustments_applied": feedback_adjustments,
        "research_timestamp": datetime.now().isoformat()
    }
    
    output = AgentOutput(
        agent_name="industry_researcher_node",
        task_id=task["task_id"],
        output_type="è¡Œä¸šæŠ¥å‘Š",
        content=result,
        confidence_score=0.85 + (0.05 * iteration_count),
        data_sources=["ç™¾ç‚¼API", "MCP API", "è¡Œä¸šæ•°æ®åº“", "ç”¨æˆ·åé¦ˆ"],
        analysis_method="LLMåˆ†æ+æ•°æ®æŒ–æ˜+åé¦ˆä¼˜åŒ–",
        timestamp=datetime.now(),
        quality_metrics={"completeness": 0.9, "timeliness": 0.95, "relevance": 0.8 + (0.1 * iteration_count)},
        recommendations=result.get("recommendations", []),
        warnings=None
    )
    
    updates = {
        "industry_research_result": result, 
        "agent_outputs": [output]  # è¿”å›å•ä¸ªè¾“å‡ºï¼Œç”±Annotatedè‡ªåŠ¨åˆå¹¶
    }
    
    print(f"ğŸ”„ çŠ¶æ€æ›´æ–°: {json.dumps(updates, ensure_ascii=False, indent=2, default=str)}")
    return updates


def job_analyzer_node(state: CareerNavigatorState, config: RunnableConfig = None) -> Dict[str, Any]:
    """èŒä¸šåˆ†æèŠ‚ç‚¹ (å¹¶è¡Œ)"""
    print("=" * 60)
    print("ğŸ’¼ æ­£åœ¨æ‰§è¡Œ: job_analyzer_node")
    print("=" * 60)
    
    # è·å–æµå¼å›è°ƒ
    stream_callback = None
    if config and "configurable" in config and "stream_callback" in config["configurable"]:
        stream_callback = config["configurable"]["stream_callback"]
        if stream_callback:
            stream_callback(json.dumps({"node": "job_analyzer", "status": "start"}))

    task = next((t for t in state["agent_tasks"] if t["agent_name"] == "job_analyzer_node"), None)
    
    if not task:
        print("âŒ æœªæ‰¾åˆ°èŒä¸šåˆ†æä»»åŠ¡")
        if stream_callback:
            stream_callback(json.dumps({"node": "job_analyzer", "status": "end"}))
        return StateUpdater.log_error(state, {"error": "æœªæ‰¾åˆ°èŒä¸šåˆ†æä»»åŠ¡"})
    
    print(f"ğŸ“‹ ä»»åŠ¡ä¿¡æ¯: {task['task_type']} - {task['description']}")
    
    target_career = task["input_data"].get("target_career", "äº§å“ç»ç†")
    user_profile = state["user_profile"]
    feedback_adjustments = task["input_data"].get("feedback_adjustments", {})
    iteration_count = task["input_data"].get("iteration_count", 0)
    
    print(f"ğŸ’¼ ç›®æ ‡èŒä¸š: {target_career}")
    print(f"ğŸ”„ è¿­ä»£æ¬¡æ•°: {iteration_count}")
    print(f"ğŸ¯ åé¦ˆè°ƒæ•´: {json.dumps(feedback_adjustments, ensure_ascii=False, indent=2)}")
    
    # æ„å»ºåˆ†æè¯·æ±‚ï¼Œå°†UserProfileè½¬æ¢ä¸ºdict
    analysis_request = {
        "target_career": target_career,
        "user_profile": dict(user_profile),  # è½¬æ¢ä¸ºæ™®é€šå­—å…¸
        "focus_areas": feedback_adjustments.get("focus_areas", []),
        "is_iteration": iteration_count > 0,
        "special_considerations": "ç»“åˆç”¨æˆ·åé¦ˆï¼Œé‡ç‚¹åˆ†æAIäº§å“ç»ç†ç›¸å…³çš„æŠ€èƒ½å’Œè·¯å¾„" if "AI" in str(feedback_adjustments) else ""
    }
    
    print(f"ğŸ“¤ åˆ†æè¯·æ±‚: {json.dumps(analysis_request, ensure_ascii=False, indent=2, default=str)}")
    
    # è°ƒç”¨ç™¾ç‚¼APIè¿›è¡ŒèŒä¸šåˆ†æ
    llm_response = llm_service.analyze_career_opportunities(
        target_career, 
        dict(user_profile),
        stream_callback=lambda x: stream_callback(json.dumps({"node": "job_analyzer", "content": x})) if stream_callback else None
    )
    
    if stream_callback:
        stream_callback(json.dumps({"node": "job_analyzer", "status": "end"}))
    
    print(f"ğŸ¤– LLMåŸå§‹å“åº”: {json.dumps(llm_response, ensure_ascii=False, indent=2)}")
    
    if llm_response.get("success"):
        try:
            # ä½¿ç”¨æ™ºèƒ½JSONè§£æ
            result = parse_llm_json_content(llm_response["content"])
            print(f"ğŸ“Š èŒä¸šåˆ†æç»“æœ (è¿­ä»£{iteration_count}): {json.dumps(result, ensure_ascii=False, indent=2)}")
        except json.JSONDecodeError as e:
            result = {"error": f"å“åº”è§£æå¤±è´¥: {str(e)}", "raw_response": llm_response["content"][:500]}
            print(f"âŒ å“åº”è§£æå¤±è´¥: {result}")
    else:
        result = {"error": llm_response.get("error", "åˆ†æå¤±è´¥")}
        print(f"âŒ åˆ†æå¤±è´¥: {result}")
    
    # è¡¥å……æ¨¡æ‹Ÿçš„èŒä½å¸‚åœºæ•°æ®
    mcp_data = call_mcp_api("job_market", task["input_data"])
    #print(f"ğŸ”— MCP job_market ç»“æœ: {json.dumps(mcp_data, ensure_ascii=False, indent=2)}")
    #èŒä¸šå¸‚åœºçˆ¬å–ç»“æœ
    result["job_market_data"] = mcp_data
    
    # æ·»åŠ è¿­ä»£ä¿¡æ¯
    result["iteration_info"] = {
        "iteration_count": iteration_count,
        "adjustments_applied": feedback_adjustments,
        "analysis_timestamp": datetime.now().isoformat()
    }
    
    output = AgentOutput(
        agent_name="job_analyzer_node",
        task_id=task["task_id"],
        output_type="èŒä¸šå»ºè®®",
        content=result,
        confidence_score=0.82 + (0.08 * iteration_count),
        data_sources=["ç™¾ç‚¼API", "MCP API", "æ‹›è˜ç½‘ç«™", "ç”¨æˆ·åé¦ˆ"],
        analysis_method="LLMåˆ†æ+å¸‚åœºè°ƒç ”+åé¦ˆä¼˜åŒ–",
        timestamp=datetime.now(),
        quality_metrics={"relevance": 0.9 + (0.05 * iteration_count), "accuracy": 0.8 + (0.1 * iteration_count)},
        recommendations=result.get("recommendations", []),
        warnings=result.get("risk_warnings", [])
    )
    
    updates = {
        "career_analysis_result": result, 
        "agent_outputs": [output]  # è¿”å›å•ä¸ªè¾“å‡ºï¼Œç”±Annotatedè‡ªåŠ¨åˆå¹¶
    }
    
    print(f"ğŸ”„ çŠ¶æ€æ›´æ–°: {json.dumps(updates, ensure_ascii=False, indent=2, default=str)}")
    return updates


# --- ç»“æœæ±‡æ€»ä¸è§„åˆ’èŠ‚ç‚¹ ---
def reporter_node(state: CareerNavigatorState, config: RunnableConfig = None) -> Dict[str, Any]:
    """
    æ±‡æŠ¥å‘˜èŠ‚ç‚¹
    
    èŒè´£:
    1. æ”¶é›†æ‰€æœ‰å¹¶è¡Œåˆ†æèŠ‚ç‚¹çš„ç»“æœã€‚
    2. è°ƒç”¨LLMå°†ç»“æœæ•´åˆæˆä¸€ä»½ç»“æ„åŒ–çš„ç»¼åˆæŠ¥å‘Šã€‚
    3. æ›´æ–°çŠ¶æ€ï¼Œå‡†å¤‡è¿›å…¥ç”¨æˆ·åé¦ˆé˜¶æ®µã€‚
    4. åœ¨è¿­ä»£æ—¶ï¼Œæ˜¾ç¤ºæ”¹è¿›ä¿¡æ¯ã€‚
    """
    print("=" * 60)
    print("ğŸ“Š æ­£åœ¨æ‰§è¡Œ: reporter_node")
    print("=" * 60)
    
    # è·å–æµå¼å›è°ƒ
    stream_callback = None
    if config and "configurable" in config and "stream_callback" in config["configurable"]:
        stream_callback = config["configurable"]["stream_callback"]
        if stream_callback:
            stream_callback(json.dumps({"node": "reporter", "status": "start"}))
            stream_callback(json.dumps({"node": "reporter", "content": "æ­£åœ¨æ±‡æ€»åˆ†æç»“æœå¹¶ç”Ÿæˆç»¼åˆæŠ¥å‘Š..."}))

    # æ£€æŸ¥æ‰€æœ‰åˆ†ææ˜¯å¦å·²å®Œæˆ
    required_results = ["self_insight_result", "industry_research_result", "career_analysis_result"]
    if not all(state.get(key) for key in required_results):
        print("âŒ éƒ¨åˆ†åˆ†æç»“æœç¼ºå¤±ï¼Œæ— æ³•ç”ŸæˆæŠ¥å‘Š")
        return StateUpdater.log_error(state, {"error": "éƒ¨åˆ†åˆ†æç»“æœç¼ºå¤±ï¼Œæ— æ³•ç”ŸæˆæŠ¥å‘Šã€‚"})

    analysis_results = {
        "profile_analysis": state["self_insight_result"],
        "industry_research": state["industry_research_result"],
        "career_analysis": state["career_analysis_result"]
    }
    
    print(f"ğŸ“‹ æ”¶é›†åˆ°çš„åˆ†æç»“æœ:")
    for key, value in analysis_results.items():
        print(f"   - {key}: {type(value).__name__}")
        if isinstance(value, dict) and "error" not in value:
            print(f"     æ‘˜è¦: {json.dumps(value, ensure_ascii=False)[:200]}...")
    
    # æ£€æŸ¥æ˜¯å¦ä¸ºè¿­ä»£
    iteration_count = state.get("iteration_count", 0)
    feedback_history = state.get("user_feedback_history", [])
    
    # æ·»åŠ è¿­ä»£ä¸Šä¸‹æ–‡
    if iteration_count > 0 and feedback_history:
        latest_feedback = feedback_history[-1]
        analysis_results["iteration_context"] = {
            "iteration_count": iteration_count,
            "previous_feedback": latest_feedback.get("feedback_text", ""),
            "satisfaction_level": latest_feedback.get("satisfaction_level", ""),
            "improvements_made": "åŸºäºæ‚¨çš„åé¦ˆé‡æ–°åˆ†æäº†ç›¸å…³é¢†åŸŸ"
        }
        print(f"ğŸ“ˆ ç”Ÿæˆç¬¬{iteration_count}æ¬¡è¿­ä»£æŠ¥å‘Šï¼ŒåŸºäºç”¨æˆ·åé¦ˆ: {latest_feedback.get('feedback_text', '')}")
    
    print(f"ğŸ“¤ ç»¼åˆæŠ¥å‘Šè¯·æ±‚: {json.dumps(analysis_results, ensure_ascii=False, indent=2, default=str)}")
    
    # è°ƒç”¨ç™¾ç‚¼APIç”Ÿæˆç»¼åˆæŠ¥å‘Š (ReporterèŠ‚ç‚¹ä¸éœ€è¦æµå¼è¾“å‡º)
    llm_response = llm_service.generate_integrated_report(
        analysis_results
    )
    
    print(f"ğŸ¤– LLMåŸå§‹å“åº”: {json.dumps(llm_response, ensure_ascii=False, indent=2)}")
    
    if llm_response.get("success"):
        try:
            # ä½¿ç”¨æ™ºèƒ½JSONè§£æ
            report = parse_llm_json_content(llm_response["content"])
            if iteration_count > 0:
                report["iteration_summary"] = f"è¿™æ˜¯åŸºäºæ‚¨åé¦ˆçš„ç¬¬{iteration_count}æ¬¡ä¼˜åŒ–æŠ¥å‘Š"
            print(f"ğŸ“Š ç»¼åˆæŠ¥å‘Šç”ŸæˆæˆåŠŸ (è¿­ä»£{iteration_count}): {json.dumps(report, ensure_ascii=False, indent=2)}")
        except json.JSONDecodeError as e:
            report = {
                "executive_summary": "ç»¼åˆåˆ†ææŠ¥å‘Š",
                "error": f"æŠ¥å‘Šè§£æå¤±è´¥: {str(e)}",
                "raw_response": llm_response["content"][:500],
                "iteration_count": iteration_count
            }
            print(f"âŒ æŠ¥å‘Šè§£æå¤±è´¥: {report}")
    else:
        report = {
            "executive_summary": "ç»¼åˆåˆ†ææŠ¥å‘Š",
            "error": llm_response.get("error", "æŠ¥å‘Šç”Ÿæˆå¤±è´¥"),
            "iteration_count": iteration_count
        }
        print(f"âŒ æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {report}")
    
    # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•°
    iteration_count = state.get("iteration_count", 0)
    max_iterations = state.get("max_iterations", 2)
    
    if iteration_count >= max_iterations:
        print(f"âš ï¸ å·²è¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•°({max_iterations})ï¼Œè·³è¿‡ç”¨æˆ·åé¦ˆï¼Œç›´æ¥è¿›å…¥ç›®æ ‡æ‹†åˆ†é˜¶æ®µ")
        # ç›´æ¥è¿›å…¥ç›®æ ‡æ‹†åˆ†é˜¶æ®µï¼Œè·³è¿‡ç”¨æˆ·åé¦ˆ
        updated_state = StateUpdater.update_stage(state, WorkflowStage.GOAL_DECOMPOSITION)
        updated_state["integrated_report"] = report
        updated_state["skip_feedback_reason"] = "è¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•°"
        
        if stream_callback:
            stream_callback(json.dumps({"node": "reporter", "status": "end"}))
            
        print(f"ğŸ”„ çŠ¶æ€æ›´æ–°: {json.dumps(updated_state, ensure_ascii=False, indent=2, default=str)}")
        return updated_state
    else:
        print(f"ğŸ“ è¿­ä»£æ¬¡æ•°({iteration_count}/{max_iterations})ï¼Œè¿›å…¥ç”¨æˆ·åé¦ˆé˜¶æ®µ")
        # æ›´æ–°çŠ¶æ€ï¼Œè¿›å…¥ç”¨æˆ·åé¦ˆé˜¶æ®µ
        updated_state = StateUpdater.update_stage(state, WorkflowStage.USER_FEEDBACK)
        updated_state["integrated_report"] = report
        # è®¾ç½®éœ€è¦ç”¨æˆ·è¾“å…¥æ ‡å¿—ï¼Œå¹¶æå‡ºé—®é¢˜
        feedback_question = f"è¿™æ˜¯ç¬¬{iteration_count + 1}æ¬¡åˆ†ææŠ¥å‘Šï¼Œæ‚¨å¯¹è¿™ä»½ç»¼åˆæŠ¥å‘Šæ»¡æ„å—ï¼Ÿè¯·æä¾›æ‚¨çš„åé¦ˆæˆ–ä¿®æ”¹æ„è§ã€‚" if iteration_count > 0 else "æ‚¨å¯¹è¿™ä»½ç»¼åˆæŠ¥å‘Šæ»¡æ„å—ï¼Ÿè¯·æä¾›æ‚¨çš„åé¦ˆæˆ–ä¿®æ”¹æ„è§ã€‚"
        updated_state.update(StateUpdater.set_user_input_required(
            state, True, [feedback_question]
        ))
        
        if stream_callback:
            stream_callback(json.dumps({"node": "reporter", "status": "end"}))
            
        print(f"ğŸ”„ çŠ¶æ€æ›´æ–°: {json.dumps(updated_state, ensure_ascii=False, indent=2, default=str)}")
        return updated_state


def goal_decomposer_node(state: CareerNavigatorState, config: RunnableConfig = None) -> Dict[str, Any]:
    """
    ç›®æ ‡æ‹†åˆ†èŠ‚ç‚¹
    
    èŒè´£:
    1. åŸºäºç”¨æˆ·ç¡®è®¤çš„èŒä¸šæ–¹å‘ï¼ˆæ¥è‡ªç»¼åˆæŠ¥å‘Šï¼‰ã€‚
    2. å°†å…¶åˆ†è§£ä¸ºé•¿æœŸã€ä¸­æœŸã€çŸ­æœŸç›®æ ‡ã€‚
    """
    print("=" * 60)
    print("ğŸ¯ æ­£åœ¨æ‰§è¡Œ: goal_decomposer_node")
    print("=" * 60)
    
    # è·å–æµå¼å›è°ƒ
    stream_callback = None
    if config and "configurable" in config and "stream_callback" in config["configurable"]:
        stream_callback = config["configurable"]["stream_callback"]
        if stream_callback:
            stream_callback(json.dumps({"node": "goal_decomposer", "status": "start"}))
            stream_callback(json.dumps({"node": "goal_decomposer", "content": "æ­£åœ¨å°†èŒä¸šç›®æ ‡æ‹†è§£ä¸ºé˜¶æ®µæ€§è®¡åˆ’..."}))

    # è·å–èŒä¸šæ–¹å‘
    integrated_report = state.get("integrated_report") or {}
    career_match = integrated_report.get("career_match") or {}
    career_direction = career_match.get("recommended_career", "")
    
    if not career_direction:
        # ä»ç”¨æˆ·ç”»åƒä¸­è·å–èŒä¸šç›®æ ‡
        user_profile = state.get("user_profile") or {}
        career_direction = user_profile.get("career_goals", "èŒä¸šå‘å±•")
    
    user_profile = state.get("user_profile") or {}
    
    print(f"ğŸ¯ ç›®æ ‡èŒä¸šæ–¹å‘: {career_direction}")
    print(f"ğŸ‘¤ ç”¨æˆ·ç”»åƒ: {json.dumps(dict(user_profile), ensure_ascii=False, indent=2)}")
    
    # è°ƒç”¨ç™¾ç‚¼APIè¿›è¡Œç›®æ ‡æ‹†åˆ†
    llm_response = llm_service.decompose_career_goals(
        career_direction, 
        user_profile,
        stream_callback=lambda x: stream_callback(json.dumps({"node": "goal_decomposer", "content": x})) if stream_callback else None
    )
    
    if stream_callback:
        stream_callback(json.dumps({"node": "goal_decomposer", "status": "end"}))
    
    print(f"ğŸ¤– LLMåŸå§‹å“åº”: {json.dumps(llm_response, ensure_ascii=False, indent=2)}")
    
    # å‡†å¤‡æ›´æ–°ï¼ŒåŒæ—¶æ¸…é™¤æ»¡æ„åº¦çŠ¶æ€ï¼Œä»¥ä¾¿ä¸‹æ¬¡åé¦ˆå¾ªç¯
    updated_state = {"current_satisfaction": None}
    
    if llm_response.get("success"):
        try:
            # ä½¿ç”¨æ™ºèƒ½JSONè§£æ
            decomposed_goals = parse_llm_json_content(llm_response["content"])
            print(f"ğŸ“Š ç›®æ ‡æ‹†åˆ†å®Œæˆ: {json.dumps(decomposed_goals, ensure_ascii=False, indent=2)}")
            print(f"   - çŸ­æœŸç›®æ ‡: {len(decomposed_goals.get('short_term_goals', []))} ä¸ª")
            print(f"   - ä¸­æœŸç›®æ ‡: {len(decomposed_goals.get('medium_term_goals', []))} ä¸ª")
            print(f"   - é•¿æœŸç›®æ ‡: {len(decomposed_goals.get('long_term_goals', []))} ä¸ª")
        except json.JSONDecodeError as e:
            decomposed_goals = {
                "error": f"ç›®æ ‡æ‹†åˆ†è§£æå¤±è´¥: {str(e)}",
                "raw_response": llm_response["content"][:500]
            }
            print(f"âŒ ç›®æ ‡æ‹†åˆ†è§£æå¤±è´¥: {decomposed_goals}")
    else:
        decomposed_goals = {
            "error": llm_response.get("error", "ç›®æ ‡æ‹†åˆ†å¤±è´¥")
        }
        print(f"âŒ ç›®æ ‡æ‹†åˆ†å¤±è´¥: {decomposed_goals}")
    
    # æ›´æ–°çŠ¶æ€ï¼Œè¿›å…¥æ—¥ç¨‹è§„åˆ’é˜¶æ®µ
    updated_state.update(StateUpdater.update_stage(state, WorkflowStage.SCHEDULE_PLANNING))
    updated_state["career_goals"] = decomposed_goals
    
    print(f"ğŸ”„ çŠ¶æ€æ›´æ–°: {json.dumps(updated_state, ensure_ascii=False, indent=2, default=str)}")
    return updated_state


def scheduler_node(state: CareerNavigatorState, config: RunnableConfig = None) -> Dict[str, Any]:
    """
    æ—¥ç¨‹è®¡åˆ’èŠ‚ç‚¹
    
    èŒè´£:
    1. å°†æ‹†åˆ†åçš„ç›®æ ‡æ•´åˆæˆå¯æ‰§è¡Œçš„ã€å¸¦æ—¶é—´çº¿çš„å…·ä½“ä»»åŠ¡ã€‚
    2. ç”Ÿæˆæœ€ç»ˆçš„è®¡åˆ’ï¼Œå¹¶å‡†å¤‡è¿›è¡Œæœ€ç»ˆç¡®è®¤ã€‚
    """
    print("=" * 60)
    print("ğŸ“… æ­£åœ¨æ‰§è¡Œ: scheduler_node")
    print("=" * 60)
    
    # è·å–æµå¼å›è°ƒ
    stream_callback = None
    if config and "configurable" in config and "stream_callback" in config["configurable"]:
        stream_callback = config["configurable"]["stream_callback"]
        if stream_callback:
            stream_callback(json.dumps({"node": "scheduler", "status": "start"}))
            stream_callback(json.dumps({"node": "scheduler", "content": "æ­£åœ¨ä¸ºæ‚¨åˆ¶å®šè¯¦ç»†çš„è¡ŒåŠ¨æ—¥ç¨‹è¡¨..."}))

    career_goals = state.get("career_goals") or {}
    user_profile = state.get("user_profile") or {}
    
    print(f"ğŸ¯ èŒä¸šç›®æ ‡: {json.dumps(career_goals, ensure_ascii=False, indent=2)}")
    print(f"ğŸ‘¤ ç”¨æˆ·ç”»åƒ: {json.dumps(dict(user_profile), ensure_ascii=False, indent=2)}")
    
    # æ„å»ºç”¨æˆ·çº¦æŸæ¡ä»¶
    user_constraints = {
        "work_experience": user_profile.get("work_experience", 0),
        "current_position": user_profile.get("current_position", ""),
        "location": user_profile.get("location", ""),
        "available_time": "ä¸šä½™æ—¶é—´",  # å¯ä»¥ä»ç”¨æˆ·è¾“å…¥ä¸­è·å–
        "budget": "ä¸­ç­‰"  # å¯ä»¥ä»ç”¨æˆ·è¾“å…¥ä¸­è·å–
    }
    
    print(f"âš™ï¸ ç”¨æˆ·çº¦æŸæ¡ä»¶: {json.dumps(user_constraints, ensure_ascii=False, indent=2)}")
    
    # è°ƒç”¨ç™¾ç‚¼APIåˆ¶å®šè¡ŒåŠ¨è®¡åˆ’
    llm_response = llm_service.create_action_schedule(
        [career_goals] if career_goals else [], 
        user_constraints,
        stream_callback=lambda x: stream_callback(json.dumps({"node": "scheduler", "content": x})) if stream_callback else None
    )
    
    if stream_callback:
        stream_callback(json.dumps({"node": "scheduler", "status": "end"}))
    
    print(f"ğŸ¤– LLMåŸå§‹å“åº”: {json.dumps(llm_response, ensure_ascii=False, indent=2)}")
    
    if llm_response.get("success"):
        try:
            # ä½¿ç”¨æ™ºèƒ½JSONè§£æ
            final_schedule = parse_llm_json_content(llm_response["content"])
            print(f"ğŸ“Š è¡ŒåŠ¨è®¡åˆ’åˆ¶å®šå®Œæˆ: {json.dumps(final_schedule, ensure_ascii=False, indent=2)}")
            print(f"   - è®¡åˆ’æ¦‚è¿°: {final_schedule.get('schedule_overview', 'è®¡åˆ’å·²ç”Ÿæˆ')}")
        except json.JSONDecodeError as e:
            final_schedule = {
                "error": f"è®¡åˆ’è§£æå¤±è´¥: {str(e)}",
                "raw_response": llm_response["content"][:500]
            }
            print(f"âŒ è®¡åˆ’è§£æå¤±è´¥: {final_schedule}")
    else:
        final_schedule = {
            "error": llm_response.get("error", "è®¡åˆ’åˆ¶å®šå¤±è´¥")
        }
        print(f"âŒ è®¡åˆ’åˆ¶å®šå¤±è´¥: {final_schedule}")
    
    # æ›´æ–°çŠ¶æ€ï¼Œè¿›å…¥æœ€ç»ˆç¡®è®¤é˜¶æ®µ
    updated_state = StateUpdater.update_stage(state, WorkflowStage.FINAL_CONFIRMATION)
    updated_state["final_career_plan"] = final_schedule  # ä½¿ç”¨ä¸interactive_workflow.pyä¸€è‡´çš„é”®å
    # å†æ¬¡è¯·æ±‚ç”¨æˆ·è¾“å…¥
    updated_state.update(StateUpdater.set_user_input_required(
        state, True, ["è¿™æ˜¯ä¸ºæ‚¨ç”Ÿæˆçš„æœ€ç»ˆè¡ŒåŠ¨è®¡åˆ’ï¼Œæ‚¨æ˜¯å¦æ»¡æ„ï¼Ÿ"]
    ))
    
    print(f"ğŸ”„ çŠ¶æ€æ›´æ–°: {json.dumps(updated_state, ensure_ascii=False, indent=2, default=str)}")
    return updated_state

