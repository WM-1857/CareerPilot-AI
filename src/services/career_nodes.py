"""
CareerNavigator LangGraph èŠ‚ç‚¹å®ç°
åŸºäºé˜¿é‡Œäº‘ç™¾ç‚¼APIçš„åŸå­åŒ–èŠ‚ç‚¹è®¾è®¡
"""

import uuid
import json
from datetime import datetime
from typing import Dict, Any, List

from src.models.career_state import (
    CareerNavigatorState, AgentTask, AgentOutput, AgentStatus, 
    WorkflowStage, StateUpdater, UserFeedback, UserSatisfactionLevel
)
from src.services.llm_service import llm_service, call_mcp_api


def coordinator_node(state: CareerNavigatorState) -> Dict[str, Any]:
    """
    åè°ƒå‘˜èŠ‚ç‚¹ (å…¥å£ç‚¹)
    
    èŒè´£:
    1. æ£€æŸ¥ç”¨æˆ·çš„åˆå§‹è¯·æ±‚ã€‚
    2. è°ƒç”¨LLMåˆ¤æ–­ç”¨æˆ·çš„èŒä¸šç›®æ ‡æ˜¯å¦å·²ç»æ˜ç¡®ã€‚
    3. æ ¹æ®åˆ¤æ–­ç»“æœï¼Œå†³å®šä¸‹ä¸€ä¸ªæµç¨‹èŠ‚ç‚¹ã€‚
    """
    print("=" * 60)
    print("ğŸš€ æ­£åœ¨æ‰§è¡Œ: coordinator_node")
    print("=" * 60)
    
    messages = state.get("messages", [])
    user_request = messages[-1].content if messages else ""
    print(f"ğŸ“ ç”¨æˆ·è¯·æ±‚: {user_request}")
    
    # å®‰å…¨è·å–ç”¨æˆ·ç”»åƒ
    user_profile = state.get("user_profile")
    if not user_profile:
        print("âŒ ç”¨æˆ·ç”»åƒä¿¡æ¯ç¼ºå¤±ï¼Œè·³è¿‡ç›®æ ‡æ˜ç¡®åº¦åˆ†æ")
        result = {
            "planning_strategy": {
                "analysis_approach": "direct_execution",
                "confidence_level": 0.5,
                "reasoning": "ç”¨æˆ·ç”»åƒä¿¡æ¯ç¼ºå¤±ï¼Œé‡‡ç”¨ç›´æ¥æ‰§è¡Œç­–ç•¥"
            }
        }
        print("ğŸ”„ coordinator_node è¿”å›å€¼:")
        print(f"ğŸ“¤ {json.dumps(result, ensure_ascii=False, indent=2)}")
        return result

    # è°ƒç”¨ç™¾ç‚¼APIåˆ†æç›®æ ‡æ˜ç¡®åº¦
    llm_response = llm_service.analyze_career_goal_clarity(
        user_request, 
        user_profile
    )
    
    print(f"ğŸ¤– LLMåŸå§‹å“åº”: {json.dumps(llm_response, ensure_ascii=False, indent=2)}")
    
    if llm_response.get("success"):
        try:
            # è§£æJSONå“åº”
            analysis = json.loads(llm_response["content"])
            is_goal_clear = analysis.get("is_goal_clear", False)
            clarity_score = analysis.get("clarity_score", 0)
            
            print(f"ğŸ“Š ç›®æ ‡æ˜ç¡®åº¦åˆ†æç»“æœ:")
            print(f"   - ç›®æ ‡æ˜¯å¦æ˜ç¡®: {is_goal_clear}")
            print(f"   - æ˜ç¡®åº¦è¯„åˆ†: {clarity_score}")
            print(f"   - è¯¦ç»†åˆ†æ: {json.dumps(analysis, ensure_ascii=False, indent=2)}")
            
            if is_goal_clear and clarity_score > 70:
                print("âœ… åˆ¤æ–­ï¼šç›®æ ‡æ˜ç¡®ï¼Œç›´æ¥è¿›å…¥ç›®æ ‡æ‹†åˆ†ã€‚")
                # æ›´æ–°çŠ¶æ€ï¼Œç›´æ¥è¿›å…¥ç›®æ ‡æ‹†åˆ†é˜¶æ®µ
                updates = StateUpdater.update_stage(state, WorkflowStage.GOAL_DECOMPOSITION)
                updates["next_node"] = "goal_decomposer"
                updates["cached_data"] = {"goal_analysis": analysis}
                
                print(f"ğŸ”„ çŠ¶æ€æ›´æ–°: {json.dumps(updates, ensure_ascii=False, indent=2, default=str)}")
                return updates
            else:
                print("ğŸ”„ åˆ¤æ–­ï¼šç›®æ ‡ä¸æ˜ç¡®ï¼Œéœ€è¦è¿›è¡Œè§„åˆ’å’Œåˆ†æã€‚")
                # æ›´æ–°çŠ¶æ€ï¼Œè¿›å…¥ç­–ç•¥åˆ¶å®šé˜¶æ®µ
                updates = StateUpdater.update_stage(state, WorkflowStage.PLANNING)
                updates["next_node"] = "planner"
                updates["cached_data"] = {"goal_analysis": analysis}
                
                print(f"ğŸ”„ çŠ¶æ€æ›´æ–°: {json.dumps(updates, ensure_ascii=False, indent=2, default=str)}")
                return updates
        except json.JSONDecodeError:
            print("âŒ LLMå“åº”è§£æå¤±è´¥ï¼Œé»˜è®¤è¿›å…¥è§„åˆ’é˜¶æ®µ")
            updates = StateUpdater.update_stage(state, WorkflowStage.PLANNING)
            updates["next_node"] = "planner"
            
            print(f"ğŸ”„ çŠ¶æ€æ›´æ–°: {json.dumps(updates, ensure_ascii=False, indent=2, default=str)}")
            return updates
    else:
        print(f"âŒ LLMè°ƒç”¨å¤±è´¥: {llm_response.get('error')}")
        # é»˜è®¤è¿›å…¥è§„åˆ’é˜¶æ®µ
        updates = StateUpdater.update_stage(state, WorkflowStage.PLANNING)
        updates["next_node"] = "planner"
        
        print(f"ğŸ”„ çŠ¶æ€æ›´æ–°: {json.dumps(updates, ensure_ascii=False, indent=2, default=str)}")
        return updates


def planner_node(state: CareerNavigatorState) -> Dict[str, Any]:
    """
    è®¡åˆ’å‘˜èŠ‚ç‚¹
    
    èŒè´£:
    1. å½“ç”¨æˆ·ç›®æ ‡ä¸æ˜ç¡®æ—¶ï¼Œåˆ¶å®šä¸€ä¸ªè¯¦ç»†çš„åˆ†æç­–ç•¥ã€‚
    2. å°†ç­–ç•¥å­˜å…¥Stateä¸­ï¼Œä¾›åç»­èŠ‚ç‚¹ä½¿ç”¨ã€‚
    """
    print("=" * 60)
    print("ğŸ“‹ æ­£åœ¨æ‰§è¡Œ: planner_node")
    print("=" * 60)
    
    user_profile = state["user_profile"]
    feedback_history = state["user_feedback_history"]
    
    print(f"ğŸ‘¤ ç”¨æˆ·ç”»åƒ: {json.dumps(dict(user_profile), ensure_ascii=False, indent=2)}")
    print(f"ğŸ’¬ åé¦ˆå†å²: {len(feedback_history)} æ¡è®°å½•")
    
    # è°ƒç”¨ç™¾ç‚¼APIåˆ¶å®šåˆ†æç­–ç•¥
    llm_response = llm_service.create_analysis_strategy(
        user_profile, 
        feedback_history
    )
    
    print(f"ğŸ¤– LLMåŸå§‹å“åº”: {json.dumps(llm_response, ensure_ascii=False, indent=2)}")
    
    if llm_response.get("success"):
        try:
            strategy = json.loads(llm_response["content"])
            print(f"ğŸ“Š åˆ†æç­–ç•¥ç»“æœ: {json.dumps(strategy, ensure_ascii=False, indent=2)}")
            
            updates = {"planning_strategy": strategy.get("strategy_overview", "åˆ¶å®šä¸ªæ€§åŒ–èŒä¸šåˆ†æç­–ç•¥")}
            print(f"ğŸ”„ çŠ¶æ€æ›´æ–°: {json.dumps(updates, ensure_ascii=False, indent=2)}")
            return updates
        except json.JSONDecodeError:
            print("âŒ ç­–ç•¥è§£æå¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤ç­–ç•¥")
            updates = {"planning_strategy": "åˆ¶å®šä¸ªæ€§åŒ–èŒä¸šåˆ†æç­–ç•¥"}
            print(f"ğŸ”„ çŠ¶æ€æ›´æ–°: {json.dumps(updates, ensure_ascii=False, indent=2)}")
            return updates
    else:
        print(f"âŒ ç­–ç•¥åˆ¶å®šå¤±è´¥: {llm_response.get('error')}")
        updates = {"planning_strategy": "åˆ¶å®šä¸ªæ€§åŒ–èŒä¸šåˆ†æç­–ç•¥"}
        print(f"ğŸ”„ çŠ¶æ€æ›´æ–°: {json.dumps(updates, ensure_ascii=False, indent=2)}")
        return updates


def supervisor_node(state: CareerNavigatorState) -> Dict[str, Any]:
    """
    ç®¡ç†å‘˜èŠ‚ç‚¹
    
    èŒè´£:
    1. æ ¹æ® `planning_strategy` åˆ›å»ºå¹¶åˆ†å‘å¹¶è¡Œçš„åˆ†æä»»åŠ¡ã€‚
    2. ä¸ºæ¯ä¸ªä»»åŠ¡åˆ›å»ºä¸€ä¸ª AgentTask å¯¹è±¡ï¼Œå¹¶æ·»åŠ åˆ° State ä¸­ã€‚
    3. åœ¨è¿­ä»£æ—¶ï¼Œè€ƒè™‘ç”¨æˆ·åé¦ˆæ¥è°ƒæ•´åˆ†æç­–ç•¥ã€‚
    """
    print("=" * 60)
    print("ğŸ‘¨â€ğŸ’¼ æ­£åœ¨æ‰§è¡Œ: supervisor_node")
    print("=" * 60)
    
    plan = state.get("planning_strategy", "åˆ¶å®šä¸ªæ€§åŒ–èŒä¸šåˆ†æç­–ç•¥")
    print(f"ğŸ“‹ å½“å‰ç­–ç•¥: {plan}")
    
    # æ£€æŸ¥æ˜¯å¦æœ‰ç”¨æˆ·åé¦ˆéœ€è¦è€ƒè™‘
    feedback_history = state.get("user_feedback_history", [])
    latest_feedback = feedback_history[-1] if feedback_history else None
    
    # å¦‚æœæœ‰æœ€æ–°åé¦ˆï¼Œè°ƒæ•´åˆ†æé‡ç‚¹
    analysis_adjustments = {}
    if latest_feedback:
        feedback_text = latest_feedback.get("feedback_text") or ""
        print(f"ğŸ’¬ è€ƒè™‘ç”¨æˆ·åé¦ˆè¿›è¡Œè°ƒæ•´: {feedback_text}")
        
        # æ ¹æ®åé¦ˆè°ƒæ•´åˆ†æé‡ç‚¹
        if feedback_text and ("å¤§æ¨¡å‹" in feedback_text or "AI" in feedback_text):
            analysis_adjustments["focus_areas"] = ["AIæŠ€æœ¯èƒŒæ™¯", "å¤§æ¨¡å‹ç›¸å…³ç»éªŒ", "æŠ€æœ¯è½¬äº§å“è·¯å¾„"]
        if feedback_text and "å­¦ä¹ " in feedback_text:
            analysis_adjustments["focus_areas"] = analysis_adjustments.get("focus_areas", []) + ["å­¦ä¹ è·¯å¾„", "æŠ€èƒ½æå‡"]
    
    print(f"ğŸ¯ åˆ†æè°ƒæ•´: {json.dumps(analysis_adjustments, ensure_ascii=False, indent=2)}")
    
    # åŸºäºè®¡åˆ’å’Œåé¦ˆï¼Œåˆ›å»ºä¸‰ä¸ªå¹¶è¡Œä»»åŠ¡
    tasks = [
        AgentTask(
            task_id=str(uuid.uuid4()),
            agent_name="user_profiler_node",
            task_type="ä¸ªäººåˆ†æ",
            priority=1,
            description="æ‰§è¡Œè‡ªæˆ‘æ´å¯Ÿåˆ†æï¼Œç”Ÿæˆä¸ªäººèƒ½åŠ›ç”»åƒã€‚æ ¹æ®ç”¨æˆ·åé¦ˆé‡ç‚¹åˆ†æç›¸å…³æŠ€èƒ½ã€‚",
            input_data={
                "user_profile": state["user_profile"],
                "feedback_adjustments": analysis_adjustments,
                "iteration_count": state.get("iteration_count", 0)
            },
            status=AgentStatus.IDLE,
            created_at=datetime.now(),
            deadline=None,
            dependencies=None,
            started_at=None,
            completed_at=None
        ),
        AgentTask(
            task_id=str(uuid.uuid4()),
            agent_name="industry_researcher_node",
            task_type="è¡Œä¸šç ”ç©¶",
            priority=1,
            description="æ‰§è¡Œè¡Œä¸šè¶‹åŠ¿åˆ†æï¼Œç”Ÿæˆè¡Œä¸šæŠ¥å‘Šã€‚ç»“åˆç”¨æˆ·åé¦ˆè°ƒæ•´ç ”ç©¶é‡ç‚¹ã€‚",
            input_data={
                "target_industry": state["user_profile"].get("industry"),
                "feedback_adjustments": analysis_adjustments,
                "iteration_count": state.get("iteration_count", 0)
            },
            status=AgentStatus.IDLE,
            created_at=datetime.now(),
            deadline=None,
            dependencies=None,
            started_at=None,
            completed_at=None
        ),
        AgentTask(
            task_id=str(uuid.uuid4()),
            agent_name="job_analyzer_node",
            task_type="èŒä¸šåˆ†æ",
            priority=1,
            description="æ‰§è¡ŒèŒä¸šä¸å²—ä½åˆ†æï¼Œç”ŸæˆèŒä¸šå»ºè®®ã€‚æ ¹æ®ç”¨æˆ·åé¦ˆè°ƒæ•´èŒä¸šè·¯å¾„åˆ†æã€‚",
            input_data={
                "target_career": state["user_profile"].get("career_goals"),
                "feedback_adjustments": analysis_adjustments,
                "iteration_count": state.get("iteration_count", 0)
            },
            status=AgentStatus.IDLE,
            created_at=datetime.now(),
            deadline=None,
            dependencies=None,
            started_at=None,
            completed_at=None
        )
    ]
    
    print(f"ğŸ“‹ åˆ›å»ºäº† {len(tasks)} ä¸ªå¹¶è¡Œä»»åŠ¡:")
    for i, task in enumerate(tasks, 1):
        print(f"   {i}. {task['agent_name']} - {task['task_type']}")
        print(f"      æè¿°: {task['description']}")
        print(f"      è¾“å…¥æ•°æ®: {json.dumps(task['input_data'], ensure_ascii=False, indent=6, default=str)}")
    
    # æ›´æ–°çŠ¶æ€ï¼Œè¿›å…¥å¹¶è¡Œåˆ†æé˜¶æ®µ
    updated_state = StateUpdater.update_stage(state, WorkflowStage.PARALLEL_ANALYSIS)
    updated_state["agent_tasks"] = tasks
    
    print(f"ğŸ”„ çŠ¶æ€æ›´æ–°: {json.dumps(updated_state, ensure_ascii=False, indent=2, default=str)}")
    return updated_state


# --- å¹¶è¡Œåˆ†æèŠ‚ç‚¹ ---
def user_profiler_node(state: CareerNavigatorState) -> Dict[str, Any]:
    """ç”¨æˆ·å»ºæ¨¡èŠ‚ç‚¹ (å¹¶è¡Œ)"""
    print("=" * 60)
    print("ğŸ‘¤ æ­£åœ¨æ‰§è¡Œ: user_profiler_node")
    print("=" * 60)
    
    task = next((t for t in state["agent_tasks"] if t["agent_name"] == "user_profiler_node"), None)
    
    if not task:
        print("âŒ æœªæ‰¾åˆ°ç”¨æˆ·ç”»åƒåˆ†æä»»åŠ¡")
        return StateUpdater.log_error(state, {"error": "æœªæ‰¾åˆ°ç”¨æˆ·ç”»åƒåˆ†æä»»åŠ¡"})
    
    print(f"ğŸ“‹ ä»»åŠ¡ä¿¡æ¯: {task['task_type']} - {task['description']}")
    
    # è·å–åˆ†æè°ƒæ•´å’Œè¿­ä»£ä¿¡æ¯
    input_data = task["input_data"]
    feedback_adjustments = input_data.get("feedback_adjustments", {})
    iteration_count = input_data.get("iteration_count", 0)
    
    print(f"ğŸ”„ è¿­ä»£æ¬¡æ•°: {iteration_count}")
    print(f"ğŸ¯ åé¦ˆè°ƒæ•´: {json.dumps(feedback_adjustments, ensure_ascii=False, indent=2)}")
    
    # æ„å»ºåˆ†æè¯·æ±‚ï¼ŒåŒ…å«åé¦ˆè°ƒæ•´
    analysis_request = {
        **input_data,
        "focus_areas": feedback_adjustments.get("focus_areas", []),
        "is_iteration": iteration_count > 0,
        "improvement_notes": "ç»“åˆç”¨æˆ·åé¦ˆé‡æ–°åˆ†æç”¨æˆ·èƒ½åŠ›å’Œä¼˜åŠ¿"
    }
    
    print(f"ğŸ“¤ åˆ†æè¯·æ±‚: {json.dumps(analysis_request, ensure_ascii=False, indent=2, default=str)}")
    
    # è°ƒç”¨ç™¾ç‚¼APIè¿›è¡Œç”¨æˆ·ç”»åƒåˆ†æ
    llm_response = llm_service.analyze_user_profile(analysis_request)
    
    print(f"ğŸ¤– LLMåŸå§‹å“åº”: {json.dumps(llm_response, ensure_ascii=False, indent=2)}")
    
    if llm_response.get("success"):
        try:
            result = json.loads(llm_response["content"])
            print(f"ğŸ“Š ç”¨æˆ·ç”»åƒåˆ†æç»“æœ (è¿­ä»£{iteration_count}): {json.dumps(result, ensure_ascii=False, indent=2)}")
        except json.JSONDecodeError:
            result = {"error": "å“åº”è§£æå¤±è´¥", "raw_response": llm_response["content"]}
            print(f"âŒ å“åº”è§£æå¤±è´¥: {result}")
    else:
        result = {"error": llm_response.get("error", "åˆ†æå¤±è´¥")}
        print(f"âŒ åˆ†æå¤±è´¥: {result}")
    
    # æ·»åŠ è¿­ä»£ä¿¡æ¯
    result["iteration_info"] = {
        "iteration_count": iteration_count,
        "adjustments_applied": feedback_adjustments,
        "analysis_timestamp": datetime.now().isoformat()
    }
    
    output = AgentOutput(
        agent_name="user_profiler_node",
        task_id=task["task_id"],
        output_type="ä¸ªäººç”»åƒ",
        content=result,
        confidence_score=0.8 + (0.1 * iteration_count),  # è¿­ä»£æå‡ç½®ä¿¡åº¦
        data_sources=["ç™¾ç‚¼API", "ç”¨æˆ·è¾“å…¥", "ç”¨æˆ·åé¦ˆ"],
        analysis_method="LLMåˆ†æ+åé¦ˆä¼˜åŒ–",
        timestamp=datetime.now(),
        quality_metrics={"completeness": 0.9, "accuracy": 0.8 + (0.1 * iteration_count)},
        recommendations=result.get("recommendations", []),
        warnings=None
    )
    
    updates = {
        "self_insight_result": result, 
        "agent_outputs": [output]  # è¿”å›å•ä¸ªè¾“å‡ºï¼Œç”±Annotatedè‡ªåŠ¨åˆå¹¶
    }
    
    print(f"ğŸ”„ çŠ¶æ€æ›´æ–°: {json.dumps(updates, ensure_ascii=False, indent=2, default=str)}")
    return updates


def industry_researcher_node(state: CareerNavigatorState) -> Dict[str, Any]:
    """è¡Œä¸šç ”ç©¶èŠ‚ç‚¹ (å¹¶è¡Œ)"""
    print("=" * 60)
    print("ğŸ¢ æ­£åœ¨æ‰§è¡Œ: industry_researcher_node")
    print("=" * 60)
    
    task = next((t for t in state["agent_tasks"] if t["agent_name"] == "industry_researcher_node"), None)
    
    if not task:
        print("âŒ æœªæ‰¾åˆ°è¡Œä¸šç ”ç©¶ä»»åŠ¡")
        return StateUpdater.log_error(state, {"error": "æœªæ‰¾åˆ°è¡Œä¸šç ”ç©¶ä»»åŠ¡"})
    
    print(f"ğŸ“‹ ä»»åŠ¡ä¿¡æ¯: {task['task_type']} - {task['description']}")
    
    target_industry = task["input_data"].get("target_industry", "ç§‘æŠ€è¡Œä¸š")
    feedback_adjustments = task["input_data"].get("feedback_adjustments", {})
    iteration_count = task["input_data"].get("iteration_count", 0)
    
    print(f"ğŸ¢ ç›®æ ‡è¡Œä¸š: {target_industry}")
    print(f"ğŸ”„ è¿­ä»£æ¬¡æ•°: {iteration_count}")
    print(f"ğŸ¯ åé¦ˆè°ƒæ•´: {json.dumps(feedback_adjustments, ensure_ascii=False, indent=2)}")
    
    # æ„å»ºç ”ç©¶è¯·æ±‚ï¼ŒåŒ…å«åé¦ˆè°ƒæ•´
    research_request = {
        "target_industry": target_industry,
        "focus_areas": feedback_adjustments.get("focus_areas", []),
        "is_iteration": iteration_count > 0,
        "special_focus": "ç»“åˆç”¨æˆ·åé¦ˆï¼Œé‡ç‚¹å…³æ³¨AIå’Œå¤§æ¨¡å‹ç›¸å…³çš„è¡Œä¸šæœºä¼š" if "AI" in str(feedback_adjustments) else ""
    }
    
    print(f"ğŸ“¤ ç ”ç©¶è¯·æ±‚: {json.dumps(research_request, ensure_ascii=False, indent=2)}")
    
    # è°ƒç”¨ç™¾ç‚¼APIè¿›è¡Œè¡Œä¸šç ”ç©¶
    llm_response = llm_service.research_industry_trends(target_industry)
    
    print(f"ğŸ¤– LLMåŸå§‹å“åº”: {json.dumps(llm_response, ensure_ascii=False, indent=2)}")
    
    if llm_response.get("success"):
        try:
            result = json.loads(llm_response["content"])
            print(f"ğŸ“Š è¡Œä¸šç ”ç©¶ç»“æœ (è¿­ä»£{iteration_count}): {json.dumps(result, ensure_ascii=False, indent=2)}")
        except json.JSONDecodeError:
            result = {"error": "å“åº”è§£æå¤±è´¥", "raw_response": llm_response["content"]}
            print(f"âŒ å“åº”è§£æå¤±è´¥: {result}")
    else:
        result = {"error": llm_response.get("error", "ç ”ç©¶å¤±è´¥")}
        print(f"âŒ ç ”ç©¶å¤±è´¥: {result}")
    
    # è¡¥å……æ¨¡æ‹Ÿçš„å¸‚åœºæ•°æ®
    mcp_data = call_mcp_api("industry_data", task["input_data"])
    # print(f"ğŸ”— MCP industry_data ç»“æœ: {json.dumps(mcp_data, ensure_ascii=False, indent=2)}")
    #å°±ä¸šå¸‚åœºçˆ¬å–ç»“æœ
    result["market_data"] = mcp_data
    
    # æ·»åŠ è¿­ä»£ä¿¡æ¯
    result["iteration_info"] = {
        "iteration_count": iteration_count,
        "adjustments_applied": feedback_adjustments,
        "research_timestamp": datetime.now().isoformat()
    }
    
    output = AgentOutput(
        agent_name="industry_researcher_node",
        task_id=task["task_id"],
        output_type="è¡Œä¸šæŠ¥å‘Š",
        content=result,
        confidence_score=0.85 + (0.05 * iteration_count),
        data_sources=["ç™¾ç‚¼API", "MCP API", "è¡Œä¸šæ•°æ®åº“", "ç”¨æˆ·åé¦ˆ"],
        analysis_method="LLMåˆ†æ+æ•°æ®æŒ–æ˜+åé¦ˆä¼˜åŒ–",
        timestamp=datetime.now(),
        quality_metrics={"completeness": 0.9, "timeliness": 0.95, "relevance": 0.8 + (0.1 * iteration_count)},
        recommendations=result.get("recommendations", []),
        warnings=None
    )
    
    updates = {
        "industry_research_result": result, 
        "agent_outputs": [output]  # è¿”å›å•ä¸ªè¾“å‡ºï¼Œç”±Annotatedè‡ªåŠ¨åˆå¹¶
    }
    
    print(f"ğŸ”„ çŠ¶æ€æ›´æ–°: {json.dumps(updates, ensure_ascii=False, indent=2, default=str)}")
    return updates


def job_analyzer_node(state: CareerNavigatorState) -> Dict[str, Any]:
    """èŒä¸šåˆ†æèŠ‚ç‚¹ (å¹¶è¡Œ)"""
    print("=" * 60)
    print("ğŸ’¼ æ­£åœ¨æ‰§è¡Œ: job_analyzer_node")
    print("=" * 60)
    
    task = next((t for t in state["agent_tasks"] if t["agent_name"] == "job_analyzer_node"), None)
    
    if not task:
        print("âŒ æœªæ‰¾åˆ°èŒä¸šåˆ†æä»»åŠ¡")
        return StateUpdater.log_error(state, {"error": "æœªæ‰¾åˆ°èŒä¸šåˆ†æä»»åŠ¡"})
    
    print(f"ğŸ“‹ ä»»åŠ¡ä¿¡æ¯: {task['task_type']} - {task['description']}")
    
    target_career = task["input_data"].get("target_career", "äº§å“ç»ç†")
    user_profile = state["user_profile"]
    feedback_adjustments = task["input_data"].get("feedback_adjustments", {})
    iteration_count = task["input_data"].get("iteration_count", 0)
    
    print(f"ğŸ’¼ ç›®æ ‡èŒä¸š: {target_career}")
    print(f"ğŸ”„ è¿­ä»£æ¬¡æ•°: {iteration_count}")
    print(f"ğŸ¯ åé¦ˆè°ƒæ•´: {json.dumps(feedback_adjustments, ensure_ascii=False, indent=2)}")
    
    # æ„å»ºåˆ†æè¯·æ±‚ï¼Œå°†UserProfileè½¬æ¢ä¸ºdict
    analysis_request = {
        "target_career": target_career,
        "user_profile": dict(user_profile),  # è½¬æ¢ä¸ºæ™®é€šå­—å…¸
        "focus_areas": feedback_adjustments.get("focus_areas", []),
        "is_iteration": iteration_count > 0,
        "special_considerations": "ç»“åˆç”¨æˆ·åé¦ˆï¼Œé‡ç‚¹åˆ†æAIäº§å“ç»ç†ç›¸å…³çš„æŠ€èƒ½å’Œè·¯å¾„" if "AI" in str(feedback_adjustments) else ""
    }
    
    print(f"ğŸ“¤ åˆ†æè¯·æ±‚: {json.dumps(analysis_request, ensure_ascii=False, indent=2, default=str)}")
    
    # è°ƒç”¨ç™¾ç‚¼APIè¿›è¡ŒèŒä¸šåˆ†æ
    llm_response = llm_service.analyze_career_opportunities(target_career, dict(user_profile))
    
    print(f"ğŸ¤– LLMåŸå§‹å“åº”: {json.dumps(llm_response, ensure_ascii=False, indent=2)}")
    
    if llm_response.get("success"):
        try:
            result = json.loads(llm_response["content"])
            print(f"ğŸ“Š èŒä¸šåˆ†æç»“æœ (è¿­ä»£{iteration_count}): {json.dumps(result, ensure_ascii=False, indent=2)}")
        except json.JSONDecodeError:
            result = {"error": "å“åº”è§£æå¤±è´¥", "raw_response": llm_response["content"]}
            print(f"âŒ å“åº”è§£æå¤±è´¥: {result}")
    else:
        result = {"error": llm_response.get("error", "åˆ†æå¤±è´¥")}
        print(f"âŒ åˆ†æå¤±è´¥: {result}")
    
    # è¡¥å……æ¨¡æ‹Ÿçš„èŒä½å¸‚åœºæ•°æ®
    mcp_data = call_mcp_api("job_market", task["input_data"])
    #print(f"ğŸ”— MCP job_market ç»“æœ: {json.dumps(mcp_data, ensure_ascii=False, indent=2)}")
    #èŒä¸šå¸‚åœºçˆ¬å–ç»“æœ
    result["job_market_data"] = mcp_data
    
    # æ·»åŠ è¿­ä»£ä¿¡æ¯
    result["iteration_info"] = {
        "iteration_count": iteration_count,
        "adjustments_applied": feedback_adjustments,
        "analysis_timestamp": datetime.now().isoformat()
    }
    
    output = AgentOutput(
        agent_name="job_analyzer_node",
        task_id=task["task_id"],
        output_type="èŒä¸šå»ºè®®",
        content=result,
        confidence_score=0.82 + (0.08 * iteration_count),
        data_sources=["ç™¾ç‚¼API", "MCP API", "æ‹›è˜ç½‘ç«™", "ç”¨æˆ·åé¦ˆ"],
        analysis_method="LLMåˆ†æ+å¸‚åœºè°ƒç ”+åé¦ˆä¼˜åŒ–",
        timestamp=datetime.now(),
        quality_metrics={"relevance": 0.9 + (0.05 * iteration_count), "accuracy": 0.8 + (0.1 * iteration_count)},
        recommendations=result.get("recommendations", []),
        warnings=result.get("risk_warnings", [])
    )
    
    updates = {
        "career_analysis_result": result, 
        "agent_outputs": [output]  # è¿”å›å•ä¸ªè¾“å‡ºï¼Œç”±Annotatedè‡ªåŠ¨åˆå¹¶
    }
    
    print(f"ğŸ”„ çŠ¶æ€æ›´æ–°: {json.dumps(updates, ensure_ascii=False, indent=2, default=str)}")
    return updates


# --- ç»“æœæ±‡æ€»ä¸è§„åˆ’èŠ‚ç‚¹ ---
def reporter_node(state: CareerNavigatorState) -> Dict[str, Any]:
    """
    æ±‡æŠ¥å‘˜èŠ‚ç‚¹
    
    èŒè´£:
    1. æ”¶é›†æ‰€æœ‰å¹¶è¡Œåˆ†æèŠ‚ç‚¹çš„ç»“æœã€‚
    2. è°ƒç”¨LLMå°†ç»“æœæ•´åˆæˆä¸€ä»½ç»“æ„åŒ–çš„ç»¼åˆæŠ¥å‘Šã€‚
    3. æ›´æ–°çŠ¶æ€ï¼Œå‡†å¤‡è¿›å…¥ç”¨æˆ·åé¦ˆé˜¶æ®µã€‚
    4. åœ¨è¿­ä»£æ—¶ï¼Œæ˜¾ç¤ºæ”¹è¿›ä¿¡æ¯ã€‚
    """
    print("=" * 60)
    print("ğŸ“Š æ­£åœ¨æ‰§è¡Œ: reporter_node")
    print("=" * 60)
    
    # æ£€æŸ¥æ‰€æœ‰åˆ†ææ˜¯å¦å·²å®Œæˆ
    required_results = ["self_insight_result", "industry_research_result", "career_analysis_result"]
    if not all(state.get(key) for key in required_results):
        print("âŒ éƒ¨åˆ†åˆ†æç»“æœç¼ºå¤±ï¼Œæ— æ³•ç”ŸæˆæŠ¥å‘Š")
        return StateUpdater.log_error(state, {"error": "éƒ¨åˆ†åˆ†æç»“æœç¼ºå¤±ï¼Œæ— æ³•ç”ŸæˆæŠ¥å‘Šã€‚"})

    analysis_results = {
        "profile_analysis": state["self_insight_result"],
        "industry_research": state["industry_research_result"],
        "career_analysis": state["career_analysis_result"]
    }
    
    print(f"ğŸ“‹ æ”¶é›†åˆ°çš„åˆ†æç»“æœ:")
    for key, value in analysis_results.items():
        print(f"   - {key}: {type(value).__name__}")
        if isinstance(value, dict) and "error" not in value:
            print(f"     æ‘˜è¦: {json.dumps(value, ensure_ascii=False)[:200]}...")
    
    # æ£€æŸ¥æ˜¯å¦ä¸ºè¿­ä»£
    iteration_count = state.get("iteration_count", 0)
    feedback_history = state.get("user_feedback_history", [])
    
    # æ·»åŠ è¿­ä»£ä¸Šä¸‹æ–‡
    if iteration_count > 0 and feedback_history:
        latest_feedback = feedback_history[-1]
        analysis_results["iteration_context"] = {
            "iteration_count": iteration_count,
            "previous_feedback": latest_feedback.get("feedback_text", ""),
            "satisfaction_level": latest_feedback.get("satisfaction_level", ""),
            "improvements_made": "åŸºäºæ‚¨çš„åé¦ˆé‡æ–°åˆ†æäº†ç›¸å…³é¢†åŸŸ"
        }
        print(f"ğŸ“ˆ ç”Ÿæˆç¬¬{iteration_count}æ¬¡è¿­ä»£æŠ¥å‘Šï¼ŒåŸºäºç”¨æˆ·åé¦ˆ: {latest_feedback.get('feedback_text', '')}")
    
    print(f"ğŸ“¤ ç»¼åˆæŠ¥å‘Šè¯·æ±‚: {json.dumps(analysis_results, ensure_ascii=False, indent=2, default=str)}")
    
    # è°ƒç”¨ç™¾ç‚¼APIç”Ÿæˆç»¼åˆæŠ¥å‘Š
    llm_response = llm_service.generate_integrated_report(analysis_results)
    
    print(f"ğŸ¤– LLMåŸå§‹å“åº”: {json.dumps(llm_response, ensure_ascii=False, indent=2)}")
    
    if llm_response.get("success"):
        try:
            report = json.loads(llm_response["content"])
            if iteration_count > 0:
                report["iteration_summary"] = f"è¿™æ˜¯åŸºäºæ‚¨åé¦ˆçš„ç¬¬{iteration_count}æ¬¡ä¼˜åŒ–æŠ¥å‘Š"
            print(f"ğŸ“Š ç»¼åˆæŠ¥å‘Šç”ŸæˆæˆåŠŸ (è¿­ä»£{iteration_count}): {json.dumps(report, ensure_ascii=False, indent=2)}")
        except json.JSONDecodeError:
            report = {
                "executive_summary": "ç»¼åˆåˆ†ææŠ¥å‘Š",
                "error": "æŠ¥å‘Šè§£æå¤±è´¥",
                "raw_response": llm_response["content"],
                "iteration_count": iteration_count
            }
            print(f"âŒ æŠ¥å‘Šè§£æå¤±è´¥: {report}")
    else:
        report = {
            "executive_summary": "ç»¼åˆåˆ†ææŠ¥å‘Š",
            "error": llm_response.get("error", "æŠ¥å‘Šç”Ÿæˆå¤±è´¥"),
            "iteration_count": iteration_count
        }
        print(f"âŒ æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {report}")
    
    # æ›´æ–°çŠ¶æ€ï¼Œè¿›å…¥ç”¨æˆ·åé¦ˆé˜¶æ®µ
    updated_state = StateUpdater.update_stage(state, WorkflowStage.USER_FEEDBACK)
    updated_state["integrated_report"] = report
    # è®¾ç½®éœ€è¦ç”¨æˆ·è¾“å…¥æ ‡å¿—ï¼Œå¹¶æå‡ºé—®é¢˜
    feedback_question = f"è¿™æ˜¯ç¬¬{iteration_count + 1}æ¬¡åˆ†ææŠ¥å‘Šï¼Œæ‚¨å¯¹è¿™ä»½ç»¼åˆæŠ¥å‘Šæ»¡æ„å—ï¼Ÿè¯·æä¾›æ‚¨çš„åé¦ˆæˆ–ä¿®æ”¹æ„è§ã€‚" if iteration_count > 0 else "æ‚¨å¯¹è¿™ä»½ç»¼åˆæŠ¥å‘Šæ»¡æ„å—ï¼Ÿè¯·æä¾›æ‚¨çš„åé¦ˆæˆ–ä¿®æ”¹æ„è§ã€‚"
    updated_state.update(StateUpdater.set_user_input_required(
        state, True, [feedback_question]
    ))
    
    print(f"ğŸ”„ çŠ¶æ€æ›´æ–°: {json.dumps(updated_state, ensure_ascii=False, indent=2, default=str)}")
    return updated_state


def goal_decomposer_node(state: CareerNavigatorState) -> Dict[str, Any]:
    """
    ç›®æ ‡æ‹†åˆ†èŠ‚ç‚¹
    
    èŒè´£:
    1. åŸºäºç”¨æˆ·ç¡®è®¤çš„èŒä¸šæ–¹å‘ï¼ˆæ¥è‡ªç»¼åˆæŠ¥å‘Šï¼‰ã€‚
    2. å°†å…¶åˆ†è§£ä¸ºé•¿æœŸã€ä¸­æœŸã€çŸ­æœŸç›®æ ‡ã€‚
    """
    print("=" * 60)
    print("ğŸ¯ æ­£åœ¨æ‰§è¡Œ: goal_decomposer_node")
    print("=" * 60)
    
    # è·å–èŒä¸šæ–¹å‘
    integrated_report = state.get("integrated_report", {})
    career_direction = integrated_report.get("career_match", {}).get("recommended_career", "")
    
    if not career_direction:
        # ä»ç”¨æˆ·ç”»åƒä¸­è·å–èŒä¸šç›®æ ‡
        career_direction = state["user_profile"].get("career_goals", "èŒä¸šå‘å±•")
    
    user_profile = state["user_profile"]
    
    print(f"ğŸ¯ ç›®æ ‡èŒä¸šæ–¹å‘: {career_direction}")
    print(f"ğŸ‘¤ ç”¨æˆ·ç”»åƒ: {json.dumps(dict(user_profile), ensure_ascii=False, indent=2)}")
    
    # è°ƒç”¨ç™¾ç‚¼APIè¿›è¡Œç›®æ ‡æ‹†åˆ†
    llm_response = llm_service.decompose_career_goals(career_direction, user_profile)
    
    print(f"ğŸ¤– LLMåŸå§‹å“åº”: {json.dumps(llm_response, ensure_ascii=False, indent=2)}")
    
    if llm_response.get("success"):
        try:
            decomposed_goals = json.loads(llm_response["content"])
            print(f"ğŸ“Š ç›®æ ‡æ‹†åˆ†å®Œæˆ: {json.dumps(decomposed_goals, ensure_ascii=False, indent=2)}")
            print(f"   - çŸ­æœŸç›®æ ‡: {len(decomposed_goals.get('short_term_goals', []))} ä¸ª")
            print(f"   - ä¸­æœŸç›®æ ‡: {len(decomposed_goals.get('medium_term_goals', []))} ä¸ª")
            print(f"   - é•¿æœŸç›®æ ‡: {len(decomposed_goals.get('long_term_goals', []))} ä¸ª")
        except json.JSONDecodeError:
            decomposed_goals = {
                "error": "ç›®æ ‡æ‹†åˆ†è§£æå¤±è´¥",
                "raw_response": llm_response["content"]
            }
            print(f"âŒ ç›®æ ‡æ‹†åˆ†è§£æå¤±è´¥: {decomposed_goals}")
    else:
        decomposed_goals = {
            "error": llm_response.get("error", "ç›®æ ‡æ‹†åˆ†å¤±è´¥")
        }
        print(f"âŒ ç›®æ ‡æ‹†åˆ†å¤±è´¥: {decomposed_goals}")
    
    # æ›´æ–°çŠ¶æ€ï¼Œè¿›å…¥æ—¥ç¨‹è§„åˆ’é˜¶æ®µ
    updated_state = StateUpdater.update_stage(state, WorkflowStage.SCHEDULE_PLANNING)
    updated_state["career_goals"] = decomposed_goals
    
    print(f"ğŸ”„ çŠ¶æ€æ›´æ–°: {json.dumps(updated_state, ensure_ascii=False, indent=2, default=str)}")
    return updated_state


def scheduler_node(state: CareerNavigatorState) -> Dict[str, Any]:
    """
    æ—¥ç¨‹è®¡åˆ’èŠ‚ç‚¹
    
    èŒè´£:
    1. å°†æ‹†åˆ†åçš„ç›®æ ‡æ•´åˆæˆå¯æ‰§è¡Œçš„ã€å¸¦æ—¶é—´çº¿çš„å…·ä½“ä»»åŠ¡ã€‚
    2. ç”Ÿæˆæœ€ç»ˆçš„è®¡åˆ’ï¼Œå¹¶å‡†å¤‡è¿›è¡Œæœ€ç»ˆç¡®è®¤ã€‚
    """
    print("=" * 60)
    print("ğŸ“… æ­£åœ¨æ‰§è¡Œ: scheduler_node")
    print("=" * 60)
    
    career_goals = state.get("career_goals", {})
    user_profile = state["user_profile"]
    
    print(f"ğŸ¯ èŒä¸šç›®æ ‡: {json.dumps(career_goals, ensure_ascii=False, indent=2)}")
    print(f"ğŸ‘¤ ç”¨æˆ·ç”»åƒ: {json.dumps(dict(user_profile), ensure_ascii=False, indent=2)}")
    
    # æ„å»ºç”¨æˆ·çº¦æŸæ¡ä»¶
    user_constraints = {
        "work_experience": user_profile.get("work_experience", 0),
        "current_position": user_profile.get("current_position", ""),
        "location": user_profile.get("location", ""),
        "available_time": "ä¸šä½™æ—¶é—´",  # å¯ä»¥ä»ç”¨æˆ·è¾“å…¥ä¸­è·å–
        "budget": "ä¸­ç­‰"  # å¯ä»¥ä»ç”¨æˆ·è¾“å…¥ä¸­è·å–
    }
    
    print(f"âš™ï¸ ç”¨æˆ·çº¦æŸæ¡ä»¶: {json.dumps(user_constraints, ensure_ascii=False, indent=2)}")
    
    # è°ƒç”¨ç™¾ç‚¼APIåˆ¶å®šè¡ŒåŠ¨è®¡åˆ’
    llm_response = llm_service.create_action_schedule(
        [career_goals] if career_goals else [], 
        user_constraints
    )
    
    print(f"ğŸ¤– LLMåŸå§‹å“åº”: {json.dumps(llm_response, ensure_ascii=False, indent=2)}")
    
    if llm_response.get("success"):
        try:
            final_schedule = json.loads(llm_response["content"])
            print(f"ğŸ“Š è¡ŒåŠ¨è®¡åˆ’åˆ¶å®šå®Œæˆ: {json.dumps(final_schedule, ensure_ascii=False, indent=2)}")
            print(f"   - è®¡åˆ’æ¦‚è¿°: {final_schedule.get('schedule_overview', 'è®¡åˆ’å·²ç”Ÿæˆ')}")
        except json.JSONDecodeError:
            final_schedule = {
                "error": "è®¡åˆ’è§£æå¤±è´¥",
                "raw_response": llm_response["content"]
            }
            print(f"âŒ è®¡åˆ’è§£æå¤±è´¥: {final_schedule}")
    else:
        final_schedule = {
            "error": llm_response.get("error", "è®¡åˆ’åˆ¶å®šå¤±è´¥")
        }
        print(f"âŒ è®¡åˆ’åˆ¶å®šå¤±è´¥: {final_schedule}")
    
    # æ›´æ–°çŠ¶æ€ï¼Œè¿›å…¥æœ€ç»ˆç¡®è®¤é˜¶æ®µ
    updated_state = StateUpdater.update_stage(state, WorkflowStage.FINAL_CONFIRMATION)
    updated_state["final_plan"] = final_schedule
    # å†æ¬¡è¯·æ±‚ç”¨æˆ·è¾“å…¥
    updated_state.update(StateUpdater.set_user_input_required(
        state, True, ["è¿™æ˜¯ä¸ºæ‚¨ç”Ÿæˆçš„æœ€ç»ˆè¡ŒåŠ¨è®¡åˆ’ï¼Œæ‚¨æ˜¯å¦æ»¡æ„ï¼Ÿ"]
    ))
    
    print(f"ğŸ”„ çŠ¶æ€æ›´æ–°: {json.dumps(updated_state, ensure_ascii=False, indent=2, default=str)}")
    return updated_state

